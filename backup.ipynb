{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is KubeFlow pipeline Auto Generator\n",
    "\n",
    "Below is the implementation of a pipeline autogenerate based on a config.yaml file. "
   ],
   "id": "1f53509921cbff1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T10:58:38.889760Z",
     "start_time": "2024-10-07T10:58:38.886572Z"
    }
   },
   "cell_type": "code",
   "source": [
    " ########### TODO s :\n",
    " # TODO: fix-bug : ERROR: Could not find a version that satisfies the requirement gzip (from versions: none)ERROR: No matching distribution found for gzip\n",
    " #TODO : Persistent Volume for pipelines\n",
    "#TODO : creating stages from a docker image\n",
    "#TODO : creating pipeline chains\n",
    "#TODO : creating conditional pipelines and pipeline chains\n",
    "#TODO : kepler and Prometheus queries\n",
    "#TODO : filtering stage only cover number-base threshold right now\n",
    "\n",
    "\n",
    "###########TODO : outputs artifacts !!!!!!!!!!!!!!!!\n",
    "######### 1. for experiment (main track paper)\n",
    "#########2. as a tool for general use (poster paper)"
   ],
   "id": "b1125dd488c56fca",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-07T10:58:38.894950Z",
     "start_time": "2024-10-07T10:58:38.893097Z"
    }
   },
   "source": "#https://www.kubeflow.org/docs/components/pipelines/legacy-v1/installation/localcluster-deployment/",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T10:58:38.904007Z",
     "start_time": "2024-10-07T10:58:38.901607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gzip\n",
    "import kfp\n",
    "import pandas as pd\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "import hashlib\n",
    "from typing import List, Dict, Optional, Any\n",
    "import yaml\n",
    "from psutil import users\n",
    "\n",
    "\n",
    "# @dsl.component(base_image='python:3.12.2')\n",
    "# def say_hello(input_path:str,output_file:str):\n",
    "#     data =pd.read_csv(input_path)\n",
    "#     if data is not None:\n",
    "#             json_data = data.to_json(orient='records')  # Convert DataFrame to JSON\n",
    "#             with gzip.open(output_file, 'wt', encoding='utf-8') as f:\n",
    "#                 f.write(json_data)\n",
    "#             return output_file "
   ],
   "id": "e3159b34d7c68f76",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T10:58:38.922485Z",
     "start_time": "2024-10-07T10:58:38.910728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# stages' function definitions\n",
    "\n",
    "# filtering\n",
    "def compare_rows(data: pd.DataFrame, column_name: str, threshold: int, operation: str) -> pd.DataFrame:\n",
    "    if operation == 'greater_than':\n",
    "        return data[data[column_name] > threshold]\n",
    "    elif operation == 'less_than':\n",
    "        return data[data[column_name] < threshold]\n",
    "    elif operation == 'equal_to':\n",
    "        return data[data[column_name] == threshold]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported operation\")\n",
    "    \n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas']\n",
    ")\n",
    "def filtering(data_path:str, operation: str, column_name: str, threshold: int):\n",
    "    \n",
    "    print(\"inside filtering\")\n",
    "    try:\n",
    "\n",
    "        data =pd.read_csv(data_path)\n",
    "        \n",
    "        if column_name not in data.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")  \n",
    "        \n",
    "        filtered_data=compare_rows(data, column_name, threshold, operation)\n",
    "        if filtered_data.empty:\n",
    "            print(\"filtered dat was Empty !!!\")\n",
    "        filtered_data.to_csv(data_path, index=False)\n",
    "        print(f\"Filtered data has been written to {data_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{data_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "        \n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas']\n",
    ")\n",
    "def anonymize_columns(data_path: str, columns_to_anonymize: list):\n",
    "    # Convert the Input dataset to a DataFrame\n",
    "    data_df = pd.read_csv(data_path)\n",
    "\n",
    "    for column in columns_to_anonymize:\n",
    "        if column in data_df.columns:\n",
    "            # Anonymize the column using SHA-256 hashing\n",
    "            data_df[column] = data_df[column].apply(lambda x: hashlib.sha256(str(x).encode()).hexdigest())\n",
    "        else:\n",
    "            print(f\"Warning: Column '{column}' not found in the data.\")\n",
    "\n",
    "        data_df.to_csv(data_path, index=False)\n",
    "        \n",
    "# Aggregation\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas','typing']\n",
    ")\n",
    "def aggregate_columns(\n",
    "    data_path: str, \n",
    "    groupby_column: str, \n",
    "    columns_to_aggregate: list,  # list[str]\n",
    "    aggregation_functions: dict  # dict[str, list[str]]\n",
    "):\n",
    "    # Convert Dataset to DataFrame\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    \n",
    "    try:\n",
    "        # Perform the aggregation\n",
    "        aggregated_data = data_df.groupby(groupby_column).agg(aggregation_functions)\n",
    "        \n",
    "        # Select only the required columns to aggregate\n",
    "        aggregated_data = aggregated_data[columns_to_aggregate]\n",
    "        \n",
    "        # Save the result to the output path\n",
    "        aggregated_data.to_csv(data_path, index=False)\n",
    "    except KeyError as e:\n",
    "        raise f\"Error: Column {e} not found in the data.\"\n",
    "    except Exception as e:\n",
    "        raise f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "# Function to compress JSON to GZIP\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas', 'gzip']\n",
    ")\n",
    "def compress_json_to_gzip(\n",
    "    data_path: str\n",
    "):\n",
    "    # Convert Dataset to DataFrame\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    \n",
    "    try:\n",
    "        # Convert DataFrame to JSON\n",
    "        json_data = data_df.to_json(orient='records')\n",
    "        \n",
    "        # Compress and write to GZIP file\n",
    "        with gzip.open(data_path, 'wt', encoding='utf-8') as f:\n",
    "            f.write(json_data)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Optionally save an empty file in case of failure\n",
    "        with open(data_path, 'w') as f:\n",
    "            f.write('')\n",
    "\n",
    "# Function to compress CSV to GZIP\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas', 'gzip']\n",
    ")\n",
    "def compress_csv_to_gzip(\n",
    "    data_path: str\n",
    "):\n",
    "    # Convert Dataset to DataFrame\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    \n",
    "    try:\n",
    "        # Compress DataFrame to CSV GZIP\n",
    "        with gzip.open(data_path, 'wt') as f:\n",
    "            data_df.to_csv(data_path, f)\n",
    "    except Exception as e:\n",
    "        raise f\"An error occurred: {e}\""
   ],
   "id": "49f531cb6310a64b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T10:58:38.931576Z",
     "start_time": "2024-10-07T10:58:38.928172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# parse the yaml file to create the kubeflow pipeline:\n",
    "\n",
    "# Deployment of the kubeflow\n",
    "class Deployment:\n",
    "    def __init__(self, namespace: str, prometheusURL: str ):\n",
    "       self.namespace = namespace\n",
    "       self.prometheusURL = prometheusURL\n",
    "\n",
    "#\n",
    "class Stage:\n",
    "    def __init__(self, name: str, type: str ,parameter: Dict[str, Any] ):\n",
    "        self.name = name\n",
    "        self.type = type\n",
    "        self.parameter = parameter\n",
    "\n",
    "#type : csv , pipeline\n",
    "#metadat = filepath or next name of pipeline to call.          \n",
    "class Datasource:\n",
    "    def __init__(self, type: str, metadata: str):\n",
    "        self.type = type\n",
    "        self.metadata = metadata\n",
    "\n",
    "# pipeline name must be unique in the whole config file\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, name:str, flow:List[str], datasource:Datasource , consumers:List[str]):\n",
    "        self.flow = flow\n",
    "        self.datasource = datasource\n",
    "        self.name = name\n",
    "        self.consumers = consumers\n",
    "        \n",
    "class PipelineConfig:\n",
    "    def __init__(self,pipelines:List[Pipeline],stages:List[Stage],deployment: Deployment):\n",
    "        self.pipelines = pipelines\n",
    "        self.stages = stages\n",
    "        self.deployment = deployment\n",
    "        "
   ],
   "id": "918821abfd141512",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T10:58:38.940426Z",
     "start_time": "2024-10-07T10:58:38.937241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to read YAML and convert to PipelineConfig\n",
    "def read_yaml_to_pipeline_config(file_path: str) -> PipelineConfig:\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "\n",
    "        # Create Stage objects from the YAML stages\n",
    "        stages = [Stage(name=stage['name'], type=stage['type'], parameter=stage['parameter']) for stage in data['stages']]\n",
    "\n",
    "        # Create Pipeline objects from the YAML pipelines\n",
    "        pipelines = []\n",
    "        for pipeline in data['pipelines']:\n",
    "            datasource = Datasource(type=pipeline['datasource']['type'], metadata=pipeline['datasource']['metadata'])\n",
    "            pipelines.append(Pipeline(name=pipeline['name'], flow=pipeline['flow'], datasource=datasource,consumers=pipeline['consumers']))\n",
    "\n",
    "        deployment_data = data.get('Deployment', {})\n",
    "        deployment = Deployment(namespace=deployment_data.get('namespace', ''),\n",
    "                                prometheusURL=deployment_data.get('prometheusURL', ''))\n",
    "        # Create PipelineConfig object\n",
    "        pipeline_config = PipelineConfig(pipelines=pipelines, stages=stages, deployment=deployment)\n",
    "\n",
    "        return pipeline_config"
   ],
   "id": "e85593ad6cbf87f4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T10:58:38.947816Z",
     "start_time": "2024-10-07T10:58:38.945813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_component_for_stage(stage: 'Stage'):\n",
    "    if stage.type == 'filtering':\n",
    "        return filtering\n",
    "    elif stage.type == 'anonymization':\n",
    "        return anonymize_columns\n",
    "    elif stage.type == 'aggregation':\n",
    "        return aggregate_columns\n",
    "    elif stage.type == 'compress_json_to_gzip':\n",
    "        return compress_json_to_gzip\n",
    "    elif stage.type == 'compress_csv_to_gzip':\n",
    "        return compress_csv_to_gzip\n",
    "    else:\n",
    "        print(f\"Error: Unknown stage type '{stage.type}'\")\n",
    "        return None"
   ],
   "id": "8f321e72cfc78058",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T10:58:38.955304Z",
     "start_time": "2024-10-07T10:58:38.952444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# creating the pipelines based on the pipelines config file\n",
    "\n",
    "def dynamic_pipeline(data_path: str, pipeline_config: PipelineConfig, pipeline: Pipeline):\n",
    "    # Process each pipeline stage dynamically\n",
    "    for stage_name in pipeline.flow:\n",
    "        # Find the stage by name\n",
    "        stage = next((s for s in pipeline_config.stages if s.name == stage_name), None)\n",
    "\n",
    "        if stage is None:\n",
    "            print(f\"Error: Stage {stage_name} not found in the configuration.\")\n",
    "            continue\n",
    "\n",
    "        # Create component for the current stage\n",
    "        component_op = create_component_for_stage(stage)\n",
    "\n",
    "        # Dynamically handle each stage\n",
    "        if stage.type == 'filtering':\n",
    "           \n",
    "                # Ensure to pass the output as an Output type\n",
    "                component_op(\n",
    "                    data_path=data_path, \n",
    "                    operation=stage.parameter['operation'], \n",
    "                    column_name=stage.parameter['column_name'], \n",
    "                    threshold=stage.parameter['threshold']\n",
    "                )\n",
    "        \n",
    "\n",
    "        elif stage.type == 'anonymization':\n",
    "                 component_op(\n",
    "                    data_path=data_path, \n",
    "                    columns_to_anonymize=stage.parameter['columns_to_anonymize']\n",
    "        )\n",
    "\n",
    "        elif stage.type == 'aggregation':\n",
    "               component_op(\n",
    "                    data_path=data_path, \n",
    "                    groupby_column=stage.parameter['groupby_column'], \n",
    "                    columns_to_aggregate=stage.parameter['columns_to_aggregate'], \n",
    "                    aggregation_functions=stage.parameter['aggregation_functions']\n",
    "        )\n",
    "\n",
    "        elif stage.type == 'compress_json_to_gzip':\n",
    "               component_op(\n",
    "                 data_path =data_path\n",
    "        )\n",
    "\n",
    "        elif stage.type == 'compress_csv_to_gzip':\n",
    "                component_op(\n",
    "                    data_path=data_path\n",
    "        )\n",
    "        else:\n",
    "            raise Exception(f\"Component for stage '{stage_name}' could not be created.\")\n",
    "\n",
    "    return data_path\n",
    "\n",
    "\n"
   ],
   "id": "de71969508f949a4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T10:58:38.961665Z",
     "start_time": "2024-10-07T10:58:38.959296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TODO: (first) pipeline datasource.type must be csv and it's path.\n",
    "\n",
    "def create_pipeline_for_each_config(client: kfp.Client):\n",
    "    pipeline_config_file = './data/pipeline_config_sample.yaml'  # Make sure this file exists in your working directory\n",
    "    pipeline_config = read_yaml_to_pipeline_config(pipeline_config_file)\n",
    "    # Iterate over every pipeline in the pipeline config\n",
    "    for pipeline in pipeline_config.pipelines:\n",
    "        # Generate a unique name for each pipeline\n",
    "        pipeline_name = f\"pipeline_{pipeline.name}\"\n",
    "\n",
    "        # Define a Kubeflow pipeline dynamically for each pipeline\n",
    "        @dsl.pipeline(\n",
    "            name=pipeline_name,\n",
    "            description=f\"Pipeline generated from config: {pipeline.name}\"\n",
    "        )\n",
    "        def kubeflow_pipeline():\n",
    "            # Call dynamic_pipeline to handle the stages of this specific pipeline\n",
    "            dynamic_pipeline(pipeline.datasource.metadata, pipeline_config, pipeline)\n",
    "\n",
    "        # Compile the pipeline\n",
    "        pipeline_file_name = pipeline_name + '.yaml'\n",
    "        kfp.compiler.Compiler().compile(pipeline_func=kubeflow_pipeline, package_path=pipeline_file_name)\n",
    "        print(f\"Pipeline '{pipeline_name}' compiled successfully.\")\n",
    "        \n",
    "        # Upload the compiled pipeline to the specified namespace\n",
    "        client.upload_pipeline(pipeline_package_path=pipeline_file_name, pipeline_name=pipeline_name)\n",
    "        print(f\"Pipeline '{pipeline_name}' uploaded successfully.\")\n",
    "\n",
    "    return \"All pipelines compiled successfully!\""
   ],
   "id": "f00f5d4bbdb782ea",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T10:58:39.320739Z",
     "start_time": "2024-10-07T10:58:38.965441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \n",
    "    client = kfp.Client() #namespace=pipeline_config.deployment.namespace\n",
    "    # Create and compile pipelines based on the configuration\n",
    "    result = create_pipeline_for_each_config(client)\n",
    "    print(result)  # Optionally print the result message\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "d044622fbaac9b78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 'pipeline_employee_data_pipeline' compiled successfully.\n"
     ]
    },
    {
     "ename": "ApiException",
     "evalue": "(409)\nReason: Conflict\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '7ec23a7f-5db2-4dc9-bb19-fa235a8976eb', 'Cache-Control': 'no-cache, private', 'Content-Length': '579', 'Content-Type': 'application/json', 'Date': 'Mon, 07 Oct 2024 10:58:39 GMT'})\nHTTP response body: {\"error_message\":\"Failed to create a pipeline and a pipeline version. The pipeline already exists.: Failed to create a pipeline and a pipeline version: Already exist error: Failed to create a new pipeline. The name pipeline_employee_data_pipeline already exists. Please specify a new name\",\"error_details\":\"Failed to create a pipeline and a pipeline version. The pipeline already exists.: Failed to create a pipeline and a pipeline version: Already exist error: Failed to create a new pipeline. The name pipeline_employee_data_pipeline already exists. Please specify a new name\"}\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mApiException\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Call the main function\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 10\u001B[0m     main()\n",
      "Cell \u001B[0;32mIn[20], line 5\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m client \u001B[38;5;241m=\u001B[39m kfp\u001B[38;5;241m.\u001B[39mClient() \u001B[38;5;66;03m#namespace=pipeline_config.deployment.namespace\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Create and compile pipelines based on the configuration\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m result \u001B[38;5;241m=\u001B[39m create_pipeline_for_each_config(client)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(result)\n",
      "Cell \u001B[0;32mIn[19], line 26\u001B[0m, in \u001B[0;36mcreate_pipeline_for_each_config\u001B[0;34m(client)\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpipeline_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m compiled successfully.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m# Upload the compiled pipeline to the specified namespace\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m     client\u001B[38;5;241m.\u001B[39mupload_pipeline(pipeline_package_path\u001B[38;5;241m=\u001B[39mpipeline_file_name, pipeline_name\u001B[38;5;241m=\u001B[39mpipeline_name)\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpipeline_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m uploaded successfully.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll pipelines compiled successfully!\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp/client/client.py:1414\u001B[0m, in \u001B[0;36mClient.upload_pipeline\u001B[0;34m(self, pipeline_package_path, pipeline_name, description, namespace)\u001B[0m\n\u001B[1;32m   1412\u001B[0m     pipeline_name \u001B[38;5;241m=\u001B[39m pipeline_doc\u001B[38;5;241m.\u001B[39mpipeline_spec\u001B[38;5;241m.\u001B[39mpipeline_info\u001B[38;5;241m.\u001B[39mname\n\u001B[1;32m   1413\u001B[0m validate_pipeline_display_name(pipeline_name)\n\u001B[0;32m-> 1414\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_upload_api\u001B[38;5;241m.\u001B[39mupload_pipeline(\n\u001B[1;32m   1415\u001B[0m     pipeline_package_path,\n\u001B[1;32m   1416\u001B[0m     name\u001B[38;5;241m=\u001B[39mpipeline_name,\n\u001B[1;32m   1417\u001B[0m     description\u001B[38;5;241m=\u001B[39mdescription,\n\u001B[1;32m   1418\u001B[0m     namespace\u001B[38;5;241m=\u001B[39mnamespace)\n\u001B[1;32m   1419\u001B[0m link \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_url_prefix()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/#/pipelines/details/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mpipeline_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1420\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m auth\u001B[38;5;241m.\u001B[39mis_ipython():\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp_server_api/api/pipeline_upload_service_api.py:71\u001B[0m, in \u001B[0;36mPipelineUploadServiceApi.upload_pipeline\u001B[0;34m(self, uploadfile, **kwargs)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"upload_pipeline  # noqa: E501\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03mThis method makes a synchronous HTTP request by default. To make an\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;124;03m:rtype: V2beta1Pipeline\u001B[39;00m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     70\u001B[0m kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_return_http_data_only\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 71\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupload_pipeline_with_http_info(uploadfile, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp_server_api/api/pipeline_upload_service_api.py:170\u001B[0m, in \u001B[0;36mPipelineUploadServiceApi.upload_pipeline_with_http_info\u001B[0;34m(self, uploadfile, **kwargs)\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;66;03m# Authentication setting\u001B[39;00m\n\u001B[1;32m    168\u001B[0m auth_settings \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBearer\u001B[39m\u001B[38;5;124m'\u001B[39m]  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[0;32m--> 170\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_client\u001B[38;5;241m.\u001B[39mcall_api(\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/apis/v2beta1/pipelines/upload\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPOST\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    172\u001B[0m     path_params,\n\u001B[1;32m    173\u001B[0m     query_params,\n\u001B[1;32m    174\u001B[0m     header_params,\n\u001B[1;32m    175\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody_params,\n\u001B[1;32m    176\u001B[0m     post_params\u001B[38;5;241m=\u001B[39mform_params,\n\u001B[1;32m    177\u001B[0m     files\u001B[38;5;241m=\u001B[39mlocal_var_files,\n\u001B[1;32m    178\u001B[0m     response_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mV2beta1Pipeline\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[1;32m    179\u001B[0m     auth_settings\u001B[38;5;241m=\u001B[39mauth_settings,\n\u001B[1;32m    180\u001B[0m     async_req\u001B[38;5;241m=\u001B[39mlocal_var_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124masync_req\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m    181\u001B[0m     _return_http_data_only\u001B[38;5;241m=\u001B[39mlocal_var_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_return_http_data_only\u001B[39m\u001B[38;5;124m'\u001B[39m),  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[1;32m    182\u001B[0m     _preload_content\u001B[38;5;241m=\u001B[39mlocal_var_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_preload_content\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m    183\u001B[0m     _request_timeout\u001B[38;5;241m=\u001B[39mlocal_var_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_request_timeout\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m    184\u001B[0m     collection_formats\u001B[38;5;241m=\u001B[39mcollection_formats)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp_server_api/api_client.py:364\u001B[0m, in \u001B[0;36mApiClient.call_api\u001B[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001B[0m\n\u001B[1;32m    327\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001B[39;00m\n\u001B[1;32m    328\u001B[0m \n\u001B[1;32m    329\u001B[0m \u001B[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;124;03m    then the method will return the response directly.\u001B[39;00m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    363\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m async_req:\n\u001B[0;32m--> 364\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__call_api(resource_path, method,\n\u001B[1;32m    365\u001B[0m                            path_params, query_params, header_params,\n\u001B[1;32m    366\u001B[0m                            body, post_params, files,\n\u001B[1;32m    367\u001B[0m                            response_type, auth_settings,\n\u001B[1;32m    368\u001B[0m                            _return_http_data_only, collection_formats,\n\u001B[1;32m    369\u001B[0m                            _preload_content, _request_timeout, _host)\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool\u001B[38;5;241m.\u001B[39mapply_async(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__call_api, (resource_path,\n\u001B[1;32m    372\u001B[0m                                                method, path_params,\n\u001B[1;32m    373\u001B[0m                                                query_params,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    381\u001B[0m                                                _request_timeout,\n\u001B[1;32m    382\u001B[0m                                                _host))\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp_server_api/api_client.py:188\u001B[0m, in \u001B[0;36mApiClient.__call_api\u001B[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ApiException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    187\u001B[0m     e\u001B[38;5;241m.\u001B[39mbody \u001B[38;5;241m=\u001B[39m e\u001B[38;5;241m.\u001B[39mbody\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m six\u001B[38;5;241m.\u001B[39mPY3 \u001B[38;5;28;01melse\u001B[39;00m e\u001B[38;5;241m.\u001B[39mbody\n\u001B[0;32m--> 188\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    190\u001B[0m content_type \u001B[38;5;241m=\u001B[39m response_data\u001B[38;5;241m.\u001B[39mgetheader(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent-type\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_response \u001B[38;5;241m=\u001B[39m response_data\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp_server_api/api_client.py:181\u001B[0m, in \u001B[0;36mApiClient.__call_api\u001B[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001B[0m\n\u001B[1;32m    177\u001B[0m     url \u001B[38;5;241m=\u001B[39m _host \u001B[38;5;241m+\u001B[39m resource_path\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;66;03m# perform request and return response\u001B[39;00m\n\u001B[0;32m--> 181\u001B[0m     response_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(\n\u001B[1;32m    182\u001B[0m         method, url, query_params\u001B[38;5;241m=\u001B[39mquery_params, headers\u001B[38;5;241m=\u001B[39mheader_params,\n\u001B[1;32m    183\u001B[0m         post_params\u001B[38;5;241m=\u001B[39mpost_params, body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m    184\u001B[0m         _preload_content\u001B[38;5;241m=\u001B[39m_preload_content,\n\u001B[1;32m    185\u001B[0m         _request_timeout\u001B[38;5;241m=\u001B[39m_request_timeout)\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ApiException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    187\u001B[0m     e\u001B[38;5;241m.\u001B[39mbody \u001B[38;5;241m=\u001B[39m e\u001B[38;5;241m.\u001B[39mbody\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m six\u001B[38;5;241m.\u001B[39mPY3 \u001B[38;5;28;01melse\u001B[39;00m e\u001B[38;5;241m.\u001B[39mbody\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp_server_api/api_client.py:407\u001B[0m, in \u001B[0;36mApiClient.request\u001B[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001B[0m\n\u001B[1;32m    401\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrest_client\u001B[38;5;241m.\u001B[39mOPTIONS(url,\n\u001B[1;32m    402\u001B[0m                                     query_params\u001B[38;5;241m=\u001B[39mquery_params,\n\u001B[1;32m    403\u001B[0m                                     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    404\u001B[0m                                     _preload_content\u001B[38;5;241m=\u001B[39m_preload_content,\n\u001B[1;32m    405\u001B[0m                                     _request_timeout\u001B[38;5;241m=\u001B[39m_request_timeout)\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPOST\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 407\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrest_client\u001B[38;5;241m.\u001B[39mPOST(url,\n\u001B[1;32m    408\u001B[0m                                  query_params\u001B[38;5;241m=\u001B[39mquery_params,\n\u001B[1;32m    409\u001B[0m                                  headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    410\u001B[0m                                  post_params\u001B[38;5;241m=\u001B[39mpost_params,\n\u001B[1;32m    411\u001B[0m                                  _preload_content\u001B[38;5;241m=\u001B[39m_preload_content,\n\u001B[1;32m    412\u001B[0m                                  _request_timeout\u001B[38;5;241m=\u001B[39m_request_timeout,\n\u001B[1;32m    413\u001B[0m                                  body\u001B[38;5;241m=\u001B[39mbody)\n\u001B[1;32m    414\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPUT\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    415\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrest_client\u001B[38;5;241m.\u001B[39mPUT(url,\n\u001B[1;32m    416\u001B[0m                                 query_params\u001B[38;5;241m=\u001B[39mquery_params,\n\u001B[1;32m    417\u001B[0m                                 headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    420\u001B[0m                                 _request_timeout\u001B[38;5;241m=\u001B[39m_request_timeout,\n\u001B[1;32m    421\u001B[0m                                 body\u001B[38;5;241m=\u001B[39mbody)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp_server_api/rest.py:265\u001B[0m, in \u001B[0;36mRESTClientObject.POST\u001B[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001B[0m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mPOST\u001B[39m(\u001B[38;5;28mself\u001B[39m, url, headers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, query_params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, post_params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    264\u001B[0m          body\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, _preload_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, _request_timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPOST\u001B[39m\u001B[38;5;124m\"\u001B[39m, url,\n\u001B[1;32m    266\u001B[0m                         headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    267\u001B[0m                         query_params\u001B[38;5;241m=\u001B[39mquery_params,\n\u001B[1;32m    268\u001B[0m                         post_params\u001B[38;5;241m=\u001B[39mpost_params,\n\u001B[1;32m    269\u001B[0m                         _preload_content\u001B[38;5;241m=\u001B[39m_preload_content,\n\u001B[1;32m    270\u001B[0m                         _request_timeout\u001B[38;5;241m=\u001B[39m_request_timeout,\n\u001B[1;32m    271\u001B[0m                         body\u001B[38;5;241m=\u001B[39mbody)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp_server_api/rest.py:224\u001B[0m, in \u001B[0;36mRESTClientObject.request\u001B[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001B[0m\n\u001B[1;32m    221\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse body: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, r\u001B[38;5;241m.\u001B[39mdata)\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m299\u001B[39m:\n\u001B[0;32m--> 224\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ApiException(http_resp\u001B[38;5;241m=\u001B[39mr)\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "\u001B[0;31mApiException\u001B[0m: (409)\nReason: Conflict\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '7ec23a7f-5db2-4dc9-bb19-fa235a8976eb', 'Cache-Control': 'no-cache, private', 'Content-Length': '579', 'Content-Type': 'application/json', 'Date': 'Mon, 07 Oct 2024 10:58:39 GMT'})\nHTTP response body: {\"error_message\":\"Failed to create a pipeline and a pipeline version. The pipeline already exists.: Failed to create a pipeline and a pipeline version: Already exist error: Failed to create a new pipeline. The name pipeline_employee_data_pipeline already exists. Please specify a new name\",\"error_details\":\"Failed to create a pipeline and a pipeline version. The pipeline already exists.: Failed to create a pipeline and a pipeline version: Already exist error: Failed to create a new pipeline. The name pipeline_employee_data_pipeline already exists. Please specify a new name\"}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# @dsl.pipeline(name= 'test', description=\"Pipeline generated from config\")\n",
    "# def hello_pipeline():\n",
    "#     print(\"inside hello_pipeline\")\n",
    "#     filtering(data_path='data/sample.csv', operation='greater_than', column_name='age', threshold='30')\n",
    "#     anonymize_columns(data_path='data/result.csv' , columns_to_anonymize=['name','email'])\n",
    "#     aggregate_columns(data_path='data/result.csv',groupby_column='age',columns_to_aggregate=['salary'],aggregation_functions={'salary':'sum','age':'max'})\n",
    "#     compress_json_to_gzip(data_path='data/result.csv')\n",
    "#     compress_csv_to_gzip(data_path='data/result.csv')\n",
    "#     print('after filtering')\n",
    "# def main():\n",
    "#     \n",
    "#     compiler.Compiler().compile(pipeline_func=hello_pipeline, package_path='pipeline.yaml')\n",
    "#     \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ],
   "id": "beab764c898897b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "a6e1578d95bc0648",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
