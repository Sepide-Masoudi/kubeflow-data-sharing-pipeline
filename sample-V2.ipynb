{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is KubeFlow pipeline Auto Generator\n",
    "\n",
    "Below is the implementation of a pipeline autogenerate based on a config.yaml file. \n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:47:34.094885Z",
     "start_time": "2024-10-01T15:47:31.578874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from numpy import number\n",
    "!pip install kfp pandas pyyaml\n",
    "!pip install --upgrade kfp\n"
   ],
   "id": "9f4bc6d9acff50d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (2.9.0)\r\n",
      "Requirement already satisfied: pandas in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: pyyaml in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (6.0.1)\r\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (8.1.7)\r\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (0.16)\r\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (2.20.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (2.35.0)\r\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (2.18.2)\r\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.4.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (0.4.0)\r\n",
      "Requirement already satisfied: kfp-server-api<2.4.0,>=2.1.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (2.3.0)\r\n",
      "Requirement already satisfied: kubernetes<31,>=8.0.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (25.3.0)\r\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (4.25.5)\r\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (0.10.1)\r\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (0.9.0)\r\n",
      "Requirement already satisfied: urllib3<2.0.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (1.26.20)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.65.0)\r\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.24.0)\r\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.31.0)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-auth<3,>=1.6.1->kfp) (5.5.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-auth<3,>=1.6.1->kfp) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\r\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.4.1)\r\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.2)\r\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (1.6.0)\r\n",
      "Requirement already satisfied: six>=1.10 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (1.16.0)\r\n",
      "Requirement already satisfied: certifi in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (2024.8.30)\r\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kubernetes<31,>=8.0.0->kfp) (68.2.2)\r\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kubernetes<31,>=8.0.0->kfp) (1.8.0)\r\n",
      "Requirement already satisfied: requests-oauthlib in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kubernetes<31,>=8.0.0->kfp) (2.0.0)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.6.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.4)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from requests-oauthlib->kubernetes<31,>=8.0.0->kfp) (3.2.2)\r\n",
      "Requirement already satisfied: kfp in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (2.9.0)\r\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (8.1.7)\r\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (0.16)\r\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (2.20.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (2.35.0)\r\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (2.18.2)\r\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.4.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (0.4.0)\r\n",
      "Requirement already satisfied: kfp-server-api<2.4.0,>=2.1.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (2.3.0)\r\n",
      "Requirement already satisfied: kubernetes<31,>=8.0.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (25.3.0)\r\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (4.25.5)\r\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (6.0.1)\r\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (0.10.1)\r\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (0.9.0)\r\n",
      "Requirement already satisfied: urllib3<2.0.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp) (1.26.20)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.65.0)\r\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.24.0)\r\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.31.0)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-auth<3,>=1.6.1->kfp) (5.5.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-auth<3,>=1.6.1->kfp) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\r\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.4.1)\r\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.2)\r\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (1.6.0)\r\n",
      "Requirement already satisfied: six>=1.10 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (1.16.0)\r\n",
      "Requirement already satisfied: certifi in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (2024.8.30)\r\n",
      "Requirement already satisfied: python-dateutil in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp) (2.9.0.post0)\r\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kubernetes<31,>=8.0.0->kfp) (68.2.2)\r\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kubernetes<31,>=8.0.0->kfp) (1.8.0)\r\n",
      "Requirement already satisfied: requests-oauthlib in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from kubernetes<31,>=8.0.0->kfp) (2.0.0)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.6.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.4)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (from requests-oauthlib->kubernetes<31,>=8.0.0->kfp) (3.2.2)\r\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:47:34.100472Z",
     "start_time": "2024-10-01T15:47:34.098305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.v2.dsl import Input, Output, Dataset\n",
    "import string\n",
    "import hashlib\n",
    "import gzip\n",
    "from typing import List, Dict, Optional, Any\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import pyarrow.csv as pv"
   ],
   "id": "4e7e0b6ccf7f9597",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:47:34.155176Z",
     "start_time": "2024-10-01T15:47:34.105035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# stages' function definitions\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.7\",\n",
    "    packages_to_install=['pyarrow']\n",
    ")\n",
    "# filtering\n",
    "def filtering(\n",
    "    data: str ,  # Input dataset Input[Dataset]\n",
    "    operation: str, \n",
    "    column_name: str, \n",
    "    threshold: number,\n",
    "    filtered_output: str  # Output dataset\n",
    ") :\n",
    "    \n",
    "\n",
    "    # Read the input dataset\n",
    "    df =pv.read_csv(data)\n",
    "\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
    "    \n",
    "    # Perform filtering based on the specified operation\n",
    "    if operation == 'greater_than':\n",
    "        filtered_data = df[df[column_name] > float(threshold)]\n",
    "    elif operation == 'less_than':\n",
    "        filtered_data = df[df[column_name] < float(threshold)]\n",
    "    elif operation == 'equal_to':\n",
    "        filtered_data = df[df[column_name] == float(threshold)]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported operation\")\n",
    "\n",
    "    # Save the filtered DataFrame as a new CSV file\n",
    "    #filtered_data.to_csv(filtered_output.path, index=False)\n",
    "    pv.write_csv(filtered_data, output_file=filtered_output)\n",
    "\n",
    "# Anonymization\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.7\",\n",
    "    packages_to_install=['pandas']\n",
    ")\n",
    "def anonymize_columns(\n",
    "    data: Input[Dataset], \n",
    "    columns_to_anonymize: list,  # Update typing for list of columns\n",
    "    anonymized_output: Output[Dataset]  # Output dataset\n",
    "):\n",
    "    # Convert the Input dataset to a DataFrame\n",
    "    data_df = pv.read_csv(data.path)\n",
    "\n",
    "    for column in columns_to_anonymize:\n",
    "        if column in data_df.columns:\n",
    "            # Anonymize the column using SHA-256 hashing\n",
    "            data_df[column] = data_df[column].apply(lambda x: hashlib.sha256(str(x).encode()).hexdigest())\n",
    "        else:\n",
    "            print(f\"Warning: Column '{column}' not found in the data.\")\n",
    "\n",
    "    # Save the modified DataFrame to the output path\n",
    "        pv.write_csv(data_df, anonymized_output.path)\n",
    "\n",
    "\n",
    "\n",
    "# Aggregation\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.7\",\n",
    "    packages_to_install=['pandas']\n",
    ")\n",
    "def aggregate_columns(\n",
    "    data: Input[Dataset], \n",
    "    groupby_column: str, \n",
    "    columns_to_aggregate: list,  # list[str]\n",
    "    aggregation_functions: dict,  # dict[str, list[str]]\n",
    "    aggregated_output: Output[Dataset]  # Output dataset\n",
    "):\n",
    "    # Convert Dataset to DataFrame\n",
    "    data_df = pv.read_csv(data.path)\n",
    "    \n",
    "    try:\n",
    "        # Perform the aggregation\n",
    "        aggregated_data = data_df.groupby(groupby_column).agg(aggregation_functions)\n",
    "        \n",
    "        # Select only the required columns to aggregate\n",
    "        aggregated_data = aggregated_data[columns_to_aggregate]\n",
    "        \n",
    "        # Save the result to the output path\n",
    "        pv.write_csv(aggregated_data, aggregated_output.path)\n",
    "    except KeyError as e:\n",
    "        raise f\"Error: Column {e} not found in the data.\"\n",
    "    except Exception as e:\n",
    "        raise f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "# Function to compress JSON to GZIP\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.7\",\n",
    "    packages_to_install=['pandas', 'gzip']\n",
    ")\n",
    "def compress_json_to_gzip(\n",
    "    data: Input[Dataset], \n",
    "    compressed_output: Output[Dataset]  # Output dataset\n",
    "):\n",
    "    # Convert Dataset to DataFrame\n",
    "    data_df = pv.read_csv(data.path)\n",
    "    \n",
    "    try:\n",
    "        # Convert DataFrame to JSON\n",
    "        json_data = data_df.to_json(orient='records')\n",
    "        \n",
    "        # Compress and write to GZIP file\n",
    "        with gzip.open(compressed_output.path, 'wt', encoding='utf-8') as f:\n",
    "            f.write(json_data)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Optionally save an empty file in case of failure\n",
    "        with open(compressed_output.path, 'w') as f:\n",
    "            f.write('')\n",
    "\n",
    "# Function to compress CSV to GZIP\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.7\",\n",
    "    packages_to_install=['pandas', 'gzip']\n",
    ")\n",
    "def compress_csv_to_gzip(\n",
    "    data: Input[Dataset], \n",
    "    compressed_output: Output[Dataset]  # Output dataset\n",
    "):\n",
    "    # Convert Dataset to DataFrame\n",
    "    data_df = pv.read_csv(data.path)\n",
    "    \n",
    "    try:\n",
    "        # Compress DataFrame to CSV GZIP\n",
    "        with gzip.open(compressed_output.path, 'wt') as f:\n",
    "            pv.write_csv(data_df, f)\n",
    "    except Exception as e:\n",
    "        raise f\"An error occurred: {e}\"\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Artifacts must have both a schema_title and a schema_version, separated by `@`. Got: number",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# stages' function definitions\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;129m@dsl\u001B[39m\u001B[38;5;241m.\u001B[39mcomponent(\n\u001B[1;32m      4\u001B[0m     base_image\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython:3.7\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m     packages_to_install\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpyarrow\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      6\u001B[0m )\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# filtering\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfiltering\u001B[39m(\n\u001B[1;32m      9\u001B[0m     data: \u001B[38;5;28mstr\u001B[39m ,  \u001B[38;5;66;03m# Input dataset Input[Dataset]\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     operation: \u001B[38;5;28mstr\u001B[39m, \n\u001B[1;32m     11\u001B[0m     column_name: \u001B[38;5;28mstr\u001B[39m, \n\u001B[1;32m     12\u001B[0m     threshold: number,\n\u001B[1;32m     13\u001B[0m     filtered_output: \u001B[38;5;28mstr\u001B[39m  \u001B[38;5;66;03m# Output dataset\u001B[39;00m\n\u001B[1;32m     14\u001B[0m ) :\n\u001B[1;32m     15\u001B[0m     \n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m     \u001B[38;5;66;03m# Read the input dataset\u001B[39;00m\n\u001B[1;32m     18\u001B[0m     df \u001B[38;5;241m=\u001B[39mpv\u001B[38;5;241m.\u001B[39mread_csv(data)\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m column_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp/dsl/component_decorator.py:121\u001B[0m, in \u001B[0;36mcomponent\u001B[0;34m(func, base_image, target_image, packages_to_install, pip_index_urls, output_component_file, install_kfp_package, kfp_package_path, pip_trusted_hosts)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m func \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m functools\u001B[38;5;241m.\u001B[39mpartial(\n\u001B[1;32m    111\u001B[0m         component,\n\u001B[1;32m    112\u001B[0m         base_image\u001B[38;5;241m=\u001B[39mbase_image,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    118\u001B[0m         kfp_package_path\u001B[38;5;241m=\u001B[39mkfp_package_path,\n\u001B[1;32m    119\u001B[0m         pip_trusted_hosts\u001B[38;5;241m=\u001B[39mpip_trusted_hosts)\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m component_factory\u001B[38;5;241m.\u001B[39mcreate_component_from_func(\n\u001B[1;32m    122\u001B[0m     func,\n\u001B[1;32m    123\u001B[0m     base_image\u001B[38;5;241m=\u001B[39mbase_image,\n\u001B[1;32m    124\u001B[0m     target_image\u001B[38;5;241m=\u001B[39mtarget_image,\n\u001B[1;32m    125\u001B[0m     packages_to_install\u001B[38;5;241m=\u001B[39mpackages_to_install,\n\u001B[1;32m    126\u001B[0m     pip_index_urls\u001B[38;5;241m=\u001B[39mpip_index_urls,\n\u001B[1;32m    127\u001B[0m     output_component_file\u001B[38;5;241m=\u001B[39moutput_component_file,\n\u001B[1;32m    128\u001B[0m     install_kfp_package\u001B[38;5;241m=\u001B[39minstall_kfp_package,\n\u001B[1;32m    129\u001B[0m     kfp_package_path\u001B[38;5;241m=\u001B[39mkfp_package_path,\n\u001B[1;32m    130\u001B[0m     pip_trusted_hosts\u001B[38;5;241m=\u001B[39mpip_trusted_hosts)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp/dsl/component_factory.py:573\u001B[0m, in \u001B[0;36mcreate_component_from_func\u001B[0;34m(func, base_image, target_image, packages_to_install, pip_index_urls, output_component_file, install_kfp_package, kfp_package_path, pip_trusted_hosts)\u001B[0m\n\u001B[1;32m    569\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    570\u001B[0m     command, args \u001B[38;5;241m=\u001B[39m _get_command_and_args_for_lightweight_component(\n\u001B[1;32m    571\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc)\n\u001B[0;32m--> 573\u001B[0m component_spec \u001B[38;5;241m=\u001B[39m extract_component_interface(func)\n\u001B[1;32m    574\u001B[0m component_spec\u001B[38;5;241m.\u001B[39mimplementation \u001B[38;5;241m=\u001B[39m structures\u001B[38;5;241m.\u001B[39mImplementation(\n\u001B[1;32m    575\u001B[0m     container\u001B[38;5;241m=\u001B[39mstructures\u001B[38;5;241m.\u001B[39mContainerSpecImplementation(\n\u001B[1;32m    576\u001B[0m         image\u001B[38;5;241m=\u001B[39mcomponent_image,\n\u001B[1;32m    577\u001B[0m         command\u001B[38;5;241m=\u001B[39mpackages_to_install_command \u001B[38;5;241m+\u001B[39m command,\n\u001B[1;32m    578\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m    579\u001B[0m     ))\n\u001B[1;32m    581\u001B[0m module_path \u001B[38;5;241m=\u001B[39m pathlib\u001B[38;5;241m.\u001B[39mPath(inspect\u001B[38;5;241m.\u001B[39mgetsourcefile(func))\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp/dsl/component_factory.py:437\u001B[0m, in \u001B[0;36mextract_component_interface\u001B[0;34m(func, containerized, description, name)\u001B[0m\n\u001B[1;32m    434\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    436\u001B[0m signature \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(func)\n\u001B[0;32m--> 437\u001B[0m name_to_input_spec, name_to_output_spec \u001B[38;5;241m=\u001B[39m get_name_to_specs(\n\u001B[1;32m    438\u001B[0m     signature, containerized)\n\u001B[1;32m    439\u001B[0m original_docstring \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39mgetdoc(func)\n\u001B[1;32m    440\u001B[0m parsed_docstring \u001B[38;5;241m=\u001B[39m docstring_parser\u001B[38;5;241m.\u001B[39mparse(original_docstring)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp/dsl/component_factory.py:288\u001B[0m, in \u001B[0;36mget_name_to_specs\u001B[0;34m(signature, containerized)\u001B[0m\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;66;03m# parameter type\u001B[39;00m\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    286\u001B[0m         type_string \u001B[38;5;241m=\u001B[39m type_utils\u001B[38;5;241m.\u001B[39m_annotation_to_type_struct(annotation)\n\u001B[1;32m    287\u001B[0m         name_to_input_specs[maybe_make_unique(\n\u001B[0;32m--> 288\u001B[0m             name, \u001B[38;5;28mlist\u001B[39m(name_to_input_specs))] \u001B[38;5;241m=\u001B[39m make_input_spec(\n\u001B[1;32m    289\u001B[0m                 type_string, func_param)\n\u001B[1;32m    291\u001B[0m \u001B[38;5;66;03m### handle return annotations ###\u001B[39;00m\n\u001B[1;32m    292\u001B[0m return_ann \u001B[38;5;241m=\u001B[39m signature\u001B[38;5;241m.\u001B[39mreturn_annotation\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp/dsl/component_factory.py:392\u001B[0m, in \u001B[0;36mmake_input_spec\u001B[0;34m(annotation, inspect_param)\u001B[0m\n\u001B[1;32m    387\u001B[0m default \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m inspect_param\u001B[38;5;241m.\u001B[39mdefault \u001B[38;5;241m==\u001B[39m inspect\u001B[38;5;241m.\u001B[39mParameter\u001B[38;5;241m.\u001B[39mempty \u001B[38;5;129;01mor\u001B[39;00m type_annotations\u001B[38;5;241m.\u001B[39missubclass_of_artifact(\n\u001B[1;32m    388\u001B[0m     annotation) \u001B[38;5;28;01melse\u001B[39;00m inspect_param\u001B[38;5;241m.\u001B[39mdefault\n\u001B[1;32m    390\u001B[0m optional \u001B[38;5;241m=\u001B[39m inspect_param\u001B[38;5;241m.\u001B[39mdefault \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39mParameter\u001B[38;5;241m.\u001B[39mempty \u001B[38;5;129;01mor\u001B[39;00m type_utils\u001B[38;5;241m.\u001B[39mis_task_final_status_type(\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;28mgetattr\u001B[39m(inspect_param\u001B[38;5;241m.\u001B[39mannotation, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__name__\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m--> 392\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m structures\u001B[38;5;241m.\u001B[39mInputSpec(\n\u001B[1;32m    393\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minput_output_spec_args,\n\u001B[1;32m    394\u001B[0m     default\u001B[38;5;241m=\u001B[39mdefault,\n\u001B[1;32m    395\u001B[0m     optional\u001B[38;5;241m=\u001B[39moptional,\n\u001B[1;32m    396\u001B[0m )\n",
      "File \u001B[0;32m<string>:8\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(self, type, default, optional, is_artifact_list, description)\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp/dsl/structures.py:57\u001B[0m, in \u001B[0;36mInputSpec.__post_init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__post_init__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 57\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_type()\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_usage_of_optional()\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp/dsl/structures.py:131\u001B[0m, in \u001B[0;36mInputSpec._validate_type\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;66;03m# TODO: add transformation logic so that we don't have to transform inputs at every place they are used, including v1 back compat support\u001B[39;00m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m spec_type_is_parameter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtype):\n\u001B[0;32m--> 131\u001B[0m     type_utils\u001B[38;5;241m.\u001B[39mvalidate_bundled_artifact_type(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtype)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/kfp/dsl/types/type_utils.py:544\u001B[0m, in \u001B[0;36mvalidate_bundled_artifact_type\u001B[0;34m(type_)\u001B[0m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;66;03m# two parts and neither are empty strings\u001B[39;00m\n\u001B[1;32m    543\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(split_type) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(split_type):\n\u001B[0;32m--> 544\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    545\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mArtifacts must have both a schema_title and a schema_version, separated by `@`. Got: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtype_\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    546\u001B[0m     )\n\u001B[1;32m    547\u001B[0m schema_title, schema_version \u001B[38;5;241m=\u001B[39m split_type\n\u001B[1;32m    548\u001B[0m validate_schema_title(schema_title)\n",
      "\u001B[0;31mTypeError\u001B[0m: Artifacts must have both a schema_title and a schema_version, separated by `@`. Got: number"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# parse the yaml file to create the kubeflow pipeline:\n",
    "\n",
    "# Deployment of the kubeflow\n",
    "class Deployment:\n",
    "    def __init__(self, namespace: str, prometheusURL: str ):\n",
    "       self.namespace = namespace\n",
    "       self.prometheusURL = prometheusURL\n",
    "\n",
    "#\n",
    "class Stage:\n",
    "    def __init__(self, name: str, type: str ,parameter: Dict[str, Any] ):\n",
    "        self.name = name\n",
    "        self.type = type\n",
    "        self.parameter = parameter\n",
    "\n",
    "#type : csv , pipeline\n",
    "#metadat = filepath or next name of pipeline to call.          \n",
    "class Datasource:\n",
    "    def __init__(self, type: str, metadata: str):\n",
    "        self.type = type\n",
    "        self.metadata = metadata\n",
    "\n",
    "# pipeline name must be unique in the whole config file\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, name:string, flow:List[str], datasource:Datasource):\n",
    "        self.flow = flow\n",
    "        self.datasource = datasource\n",
    "        self.name = name\n",
    "        \n",
    "class PipelineConfig:\n",
    "    def __init__(self,pipelines:List[Pipeline],stages:List[Stage],deployment: Deployment):\n",
    "        self.pipelines = pipelines\n",
    "        self.stages = stages\n",
    "        self.deployment = deployment\n",
    "        \n",
    "    "
   ],
   "id": "68f1229d10e03f02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to read YAML and convert to PipelineConfig\n",
    "def read_yaml_to_pipeline_config(file_path: str) -> PipelineConfig:\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "\n",
    "        # Create Stage objects from the YAML stages\n",
    "        stages = [Stage(name=stage['name'], type=stage['type'], parameter=stage['parameter']) for stage in data['stages']]\n",
    "\n",
    "        # Create Pipeline objects from the YAML pipelines\n",
    "        pipelines = []\n",
    "        for pipeline in data['pipelines']:\n",
    "            datasource = Datasource(type=pipeline['datasource']['type'], metadata=pipeline['datasource']['metadata'])\n",
    "            pipelines.append(Pipeline(name=pipeline['name'], flow=pipeline['flow'], datasource=datasource))\n",
    "\n",
    "        deployment_data = data.get('Deployment', {})\n",
    "        deployment = Deployment(namespace=deployment_data.get('namespace', ''),\n",
    "                                prometheusURL=deployment_data.get('prometheusURL', ''))\n",
    "        # Create PipelineConfig object\n",
    "        pipeline_config = PipelineConfig(pipelines=pipelines, stages=stages, deployment=deployment)\n",
    "\n",
    "        return pipeline_config"
   ],
   "id": "62bc5c0b4c84d649",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# kubeflow component declaration \n",
    "\n",
    "# filtering_op =kfp.components.create_component_from_func(\n",
    "#             filtering,\n",
    "#             base_image=\"python:3.7\",\n",
    "#             packages_to_install=['pandas'])\n",
    "# \n",
    "# anonymization_op =kfp.components.create_component_from_func(\n",
    "#                         anonymize_columns,\n",
    "#                         base_image=\"python:3.7\",\n",
    "#                         packages_to_install=['pandas'])\n",
    "# \n",
    "# aggregation_op = comp.create_component_from_func(\n",
    "#                 aggregate_columns,\n",
    "#                 base_image=\"python:3.7\",\n",
    "#                 packages_to_install=['pandas'])\n",
    "# \n",
    "# \n",
    "# compress_json_to_gzip_op= comp.create_component_from_func(\n",
    "#             compress_json_to_gzip,\n",
    "#             base_image=\"python:3.7\",\n",
    "#             packages_to_install=['pandas', 'gzip', 'json'])\n",
    "# \n",
    "# compress_csv_to_gzip_op= comp.create_component_from_func(\n",
    "#             compress_csv_to_gzip,\n",
    "#             base_image=\"python:3.7\",\n",
    "#             packages_to_install=['pandas', 'gzip'])"
   ],
   "id": "e94b7183597f1a9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#create pipeline components based on pipeline config object\n",
    "\n",
    "# def create_component_for_stage(stage: 'Stage') -> comp.ComponentOp:\n",
    "#     \n",
    "#     if stage.type == 'filtering':\n",
    "#       return kfp.components.create_component_from_func(\n",
    "#             filtering,\n",
    "#             base_image=\"python:3.7\",\n",
    "#             packages_to_install=['pandas'])\n",
    "#     \n",
    "#     elif stage.type == 'anonymization':\n",
    "#         return kfp.components.create_component_from_func(\n",
    "#                         anonymize_columns,\n",
    "#                         base_image=\"python:3.7\",\n",
    "#                         packages_to_install=['pandas'])\n",
    "#     \n",
    "#     elif stage.type == 'aggregation':\n",
    "#        return comp.create_component_from_func(\n",
    "#                 aggregate_columns,\n",
    "#                 base_image=\"python:3.7\",\n",
    "#                 packages_to_install=['pandas'])\n",
    "#        \n",
    "#     elif stage.type == 'compress_json_to_gzip':\n",
    "#        return comp.create_component_from_func(\n",
    "#             compress_json_to_gzip,\n",
    "#             base_image=\"python:3.7\",\n",
    "#             packages_to_install=['pandas', 'gzip', 'json'])\n",
    "#     \n",
    "#     elif stage.type == 'compress_csv_to_gzip':\n",
    "#         return comp.create_component_from_func(\n",
    "#             compress_csv_to_gzip,\n",
    "#             base_image=\"python:3.7\",\n",
    "#             packages_to_install=['pandas', 'gzip'])\n",
    "#     else:\n",
    "#         print(f\"Error: Unknown stage type '{stage.type}'\")\n",
    "#         return None  \n",
    "#   \n",
    "\n",
    "def create_component_for_stage(stage: 'Stage'):\n",
    "    if stage.type == 'filtering':\n",
    "        return filtering\n",
    "    elif stage.type == 'anonymization':\n",
    "        return anonymize_columns\n",
    "    elif stage.type == 'aggregation':\n",
    "        return aggregate_columns\n",
    "    elif stage.type == 'compress_json_to_gzip':\n",
    "        return compress_json_to_gzip\n",
    "    elif stage.type == 'compress_csv_to_gzip':\n",
    "        return compress_csv_to_gzip\n",
    "    else:\n",
    "        print(f\"Error: Unknown stage type '{stage.type}'\")\n",
    "        return None\n"
   ],
   "id": "2cd42725ed68ae26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# creating the pipelines based on the pipelines config file\n",
    "\n",
    "def dynamic_pipeline(data: Input[Dataset], pipeline_config: PipelineConfig, pipeline: Pipeline):\n",
    "    # Process each pipeline stage dynamically\n",
    "    data_op = data\n",
    "\n",
    "    for stage_name in pipeline.flow:\n",
    "        # Find the stage by name\n",
    "        stage = next((s for s in pipeline_config.stages if s.name == stage_name), None)\n",
    "\n",
    "        if stage is None:\n",
    "            print(f\"Error: Stage {stage_name} not found in the configuration.\")\n",
    "            continue\n",
    "\n",
    "        # Create component for the current stage\n",
    "        component_op = create_component_for_stage(stage)\n",
    "\n",
    "        # Dynamically handle each stage\n",
    "        if component_op:\n",
    "            # Create the output for the current stage\n",
    "            if stage.type == 'filtering':\n",
    "                filtered_output = \"/data/\"+pipeline.name+\"_\"+stage_name + \"_filtered.csv\"  \n",
    "                \n",
    "                # Ensure to pass the output as an Output type\n",
    "                data_op = component_op(\n",
    "                    data=data_op, \n",
    "                    operation=stage.parameter['operation'], \n",
    "                    column_name=stage.parameter['column_name'], \n",
    "                    threshold=stage.parameter['threshold'],\n",
    "                    # Pass the filtered_output correctly\n",
    "                    filtered_output=filtered_output\n",
    "                )\n",
    "\n",
    "            elif stage.type == 'anonymization':\n",
    "                anonymized_output = dsl.Output[Dataset](stage_name + \"_anonymized\")\n",
    "                data_op = component_op(\n",
    "                    data=data_op, \n",
    "                    columns_to_anonymize=stage.parameter['columns_to_anonymize'],\n",
    "                    anonymized_output=anonymized_output  # Pass output variable here\n",
    "                )\n",
    "\n",
    "            elif stage.type == 'aggregation':\n",
    "                aggregated_output = dsl.Output[Dataset](stage_name + \"_aggregated\")\n",
    "                data_op = component_op(\n",
    "                    data=data_op, \n",
    "                    groupby_column=stage.parameter['groupby_column'], \n",
    "                    columns_to_aggregate=stage.parameter['columns_to_aggregate'], \n",
    "                    aggregation_functions=stage.parameter['aggregation_functions'],\n",
    "                    aggregated_output=aggregated_output  # Pass output variable here\n",
    "                )\n",
    "\n",
    "            elif stage.type == 'compress_json_to_gzip':\n",
    "                compressed_output = dsl.Output[Dataset](stage_name + \"_compressed_json\")\n",
    "                data_op = component_op(\n",
    "                    data=data_op, \n",
    "                    compressed_output=compressed_output  # Pass output variable here\n",
    "                )\n",
    "\n",
    "            elif stage.type == 'compress_csv_to_gzip':\n",
    "                compressed_output = dsl.Output[Dataset](stage_name + \"_compressed_csv\")\n",
    "                data_op = component_op(\n",
    "                    data=data_op, \n",
    "                    compressed_output=compressed_output  # Pass output variable here\n",
    "                )\n",
    "        else:\n",
    "            raise Exception(f\"Component for stage '{stage_name}' could not be created.\")\n",
    "\n",
    "    return data_op\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "481a87eb74e1592b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TODO: (first) pipeline datasource.type must be csv and it's path.\n",
    "#TODO: support pipelines chains (one pipeline call the other)\n",
    "#TODO: support conditional pipeline chain and components\n",
    "\n",
    "def create_pipeline_for_each_config(pipeline_config: PipelineConfig, client: kfp.Client):\n",
    "    # Iterate over every pipeline in the pipeline config\n",
    "    for pipeline in pipeline_config.pipelines:\n",
    "        # Generate a unique name for each pipeline\n",
    "        pipeline_name = f\"pipeline_{pipeline.name}\"\n",
    "\n",
    "        # Define a Kubeflow pipeline dynamically for each pipeline\n",
    "        @dsl.pipeline(\n",
    "            name=pipeline_name,\n",
    "            description=f\"Pipeline generated from config: {pipeline.name}\"\n",
    "        )\n",
    "        def kubeflow_pipeline():\n",
    "            \n",
    "            #data = pd.read_csv(pipeline.datasource.metadata)\n",
    "            #temp_file_path = 'temp_data.csv'  # Change this to a suitable path\n",
    "            # data.to_csv(temp_file_path, index=False)\n",
    "            # if not data.empty:\n",
    "            #     dataset = Dataset(data)\n",
    "            # else:\n",
    "            #     raise ValueError(\"The provided DataFrame is empty.\")\n",
    "            data = pv.read_csv(pipeline.datasource.metadata)  # Load CSV into an Arrow Table\n",
    "            #dataset = Dataset(data) \n",
    "            # Call dynamic_pipeline to handle the stages of this specific pipeline\n",
    "            dynamic_pipeline(pipeline.datasource.metadata, pipeline_config, pipeline)\n",
    "\n",
    "        # Compile the pipeline\n",
    "        pipeline_file_name = pipeline_name + '.zip'\n",
    "        kfp.compiler.Compiler().compile(kubeflow_pipeline, pipeline_file_name)\n",
    "        print(f\"Pipeline '{pipeline_name}' compiled successfully.\")\n",
    "        \n",
    "        # Upload the compiled pipeline to the specified namespace\n",
    "        client.upload_pipeline(pipeline_file_name, name=pipeline_name)\n",
    "        print(f\"Pipeline '{pipeline_name}' uploaded successfully.\")\n",
    "\n",
    "    return \"All pipelines compiled successfully!\""
   ],
   "id": "fad906899a0dd1ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \n",
    "    # Load the pipeline configuration from a YAML file\n",
    "    pipeline_config_file = './data/pipeline_config_sample.yaml'  # Make sure this file exists in your working directory\n",
    "    pipeline_config = read_yaml_to_pipeline_config(pipeline_config_file)\n",
    "    \n",
    "    if pipeline_config.deployment.namespace is None:\n",
    "        raise Exception('please specify the kubeflow namespace in the yml file.')\n",
    "    \n",
    "    client = kfp.Client(namespace=pipeline_config.deployment.namespace)\n",
    "    # Create and compile pipelines based on the configuration\n",
    "    result = create_pipeline_for_each_config(pipeline_config, client)\n",
    "    print(result)  # Optionally print the result message\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "6836af954fe9b847",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9a6c625e97d827dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
