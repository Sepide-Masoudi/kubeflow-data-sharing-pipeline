{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is KubeFlow pipeline Auto Generator\n",
    "\n",
    "Below is the implementation of a pipeline autogenerate based on a config.yaml file. \n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T13:20:36.581724Z",
     "start_time": "2024-09-30T13:20:36.578876Z"
    }
   },
   "cell_type": "code",
   "source": "#!pip install kfp==1.8.22 pandas pyyaml",
   "id": "9f4bc6d9acff50d9",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T13:20:36.611545Z",
     "start_time": "2024-09-30T13:20:36.589665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "#from kfp.components import InputPath, OutputPath\n",
    "import pandas as pd\n",
    "import string\n",
    "import hashlib\n",
    "import gzip\n",
    "#import json\n",
    "from typing import List, Dict, Optional, Any\n",
    "import yaml\n"
   ],
   "id": "4e7e0b6ccf7f9597",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_component_from_func' from 'kfp.components' (/Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages/kfp/components/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkfp\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkfp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponents\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m create_component_from_func\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkfp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdsl\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mdsl\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#from kfp.components import InputPath, OutputPath\u001B[39;00m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'create_component_from_func' from 'kfp.components' (/Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages/kfp/components/__init__.py)"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# stages' function definitions\n",
    "\n",
    "# filtering\n",
    "def compare_rows(data: pd.DataFrame, column_name: str, threshold: str, operation: str) -> pd.DataFrame:\n",
    "    if operation == 'greater_than':\n",
    "        return data[data[column_name] > threshold]\n",
    "    elif operation == 'less_than':\n",
    "        return data[data[column_name] < threshold]\n",
    "    elif operation == 'equal_to':\n",
    "        return data[data[column_name] == threshold]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported operation\")\n",
    "\n",
    "\n",
    "# @dsl.component(\n",
    "#     base_image=\"python:3.7\",\n",
    "#     packages_to_install=['pandas']\n",
    "# )\n",
    "def filtering(data: pd.DataFrame, operation: str, column_name: str, threshold: str) -> pd.DataFrame:\n",
    "    return compare_rows(data, column_name, threshold, operation)\n",
    "\n",
    "\n",
    "# Anonymization\n",
    "# @dsl.component(\n",
    "#     base_image=\"python:3.7\",\n",
    "#     packages_to_install=['pandas']\n",
    "# )\n",
    "def anonymize_columns(data: pd.DataFrame, columns_to_anonymize: List[str]) -> pd.DataFrame:\n",
    "    for column in columns_to_anonymize:\n",
    "        if column in data.columns:\n",
    "            data[column] = data[column].apply(lambda x: hashlib.sha256(str(x).encode()).hexdigest())\n",
    "        else:\n",
    "            print(f\"Warning: Column '{column}' not found in the data.\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# Aggregation\n",
    "# @dsl.component(\n",
    "#     base_image=\"python:3.7\",\n",
    "#     packages_to_install=['pandas']\n",
    "# )\n",
    "def aggregate_columns(data: pd.DataFrame, groupby_column: str, columns_to_aggregate: List[str], aggregation_functions: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    try:\n",
    "        aggregated_data = data.groupby(groupby_column).agg(aggregation_functions)\n",
    "        return aggregated_data[columns_to_aggregate]\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column {e} not found in the data.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to compress JSON to GZIP\n",
    "# @dsl.component(\n",
    "#     base_image=\"python:3.7\",\n",
    "#     packages_to_install=['pandas', 'gzip']\n",
    "# )\n",
    "def compress_json_to_gzip(data: pd.DataFrame, output_file: str) -> Optional[str]:\n",
    "    try:\n",
    "        if data is not None:\n",
    "            json_data = data.to_json(orient='records')  # Convert DataFrame to JSON\n",
    "            with gzip.open(output_file, 'wt', encoding='utf-8') as f:\n",
    "                f.write(json_data)\n",
    "            return output_file  # Return the compressed file path\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to compress CSV to GZIP\n",
    "# @dsl.component(\n",
    "#     base_image=\"python:3.7\",\n",
    "#     packages_to_install=['pandas', 'gzip']\n",
    "# )\n",
    "def compress_csv_to_gzip(data: pd.DataFrame, output_file: str) -> Optional[str]:\n",
    "    try:\n",
    "        data.to_csv(output_file, index=False, compression='gzip')  # Compress DataFrame to CSV GZIP\n",
    "        return output_file  # Return the compressed file path\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None  \n",
    "    \n",
    "# def read_aggregate_and_convert_to_json(data: pd.DataFrame) -> Optional[str]:\n",
    "#     try:\n",
    "#         if data is not None:\n",
    "#             json_result = data.to_json(orient='records')  # Convert DataFrame to JSON\n",
    "#             return json_result\n",
    "#         else:\n",
    "#             return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         return None    \n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# parse the yaml file to create the kubeflow pipeline:\n",
    "\n",
    "# Deployment of the kubeflow\n",
    "class Deployment:\n",
    "    def __init__(self, namespace: str, prometheusURL: str ):\n",
    "       self.namespace = namespace\n",
    "       self.prometheusURL = prometheusURL\n",
    "\n",
    "#\n",
    "class Stage:\n",
    "    def __init__(self, name: str, type: str ,parameter: Dict[str, Any] ):\n",
    "        self.name = name\n",
    "        self.type = type\n",
    "        self.parameter = parameter\n",
    "\n",
    "#type : csv , pipeline\n",
    "#metadat = filepath or next name of pipeline to call.          \n",
    "class Datasource:\n",
    "    def __init__(self, type: str, metadata: str):\n",
    "        self.type = type\n",
    "        self.metadata = metadata\n",
    "\n",
    "# pipeline name must be unique in the whole config file\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, name:string, flow:List[str], datasource:Datasource):\n",
    "        self.flow = flow\n",
    "        self.datasource = datasource\n",
    "        self.name = name\n",
    "        \n",
    "class PipelineConfig:\n",
    "    def __init__(self,pipelines:List[Pipeline],stages:List[Stage],deployment: Deployment):\n",
    "        self.pipelines = pipelines\n",
    "        self.stages = stages\n",
    "        self.deployment = deployment\n",
    "        \n",
    "    "
   ],
   "id": "68f1229d10e03f02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to read YAML and convert to PipelineConfig\n",
    "def read_yaml_to_pipeline_config(file_path: str) -> PipelineConfig:\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "\n",
    "        # Create Stage objects from the YAML stages\n",
    "        stages = [Stage(name=stage['name'], type=stage['type'], parameter=stage['parameter']) for stage in data['stages']]\n",
    "\n",
    "        # Create Pipeline objects from the YAML pipelines\n",
    "        pipelines = []\n",
    "        for pipeline in data['pipelines']:\n",
    "            datasource = Datasource(type=pipeline['datasource']['type'], metadata=pipeline['datasource']['metadata'])\n",
    "            pipelines.append(Pipeline(name=pipeline['name'], flow=pipeline['flow'], datasource=datasource))\n",
    "\n",
    "        deployment_data = data.get('Deployment', {})\n",
    "        deployment = Deployment(namespace=deployment_data.get('namespace', ''),\n",
    "                                prometheusURL=deployment_data.get('prometheusURL', ''))\n",
    "        # Create PipelineConfig object\n",
    "        pipeline_config = PipelineConfig(pipelines=pipelines, stages=stages, deployment=deployment)\n",
    "\n",
    "        return pipeline_config"
   ],
   "id": "62bc5c0b4c84d649",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# kubeflow component declaration \n",
    "\n",
    "filtering_op =comp.create_component_from_func(\n",
    "            filtering,\n",
    "            base_image=\"python:3.7\",\n",
    "            packages_to_install=['pandas'])\n",
    "\n",
    "anonymization_op =comp.create_component_from_func(\n",
    "                        anonymize_columns,\n",
    "                        base_image=\"python:3.7\",\n",
    "                        packages_to_install=['pandas'])\n",
    "\n",
    "aggregation_op = comp.create_component_from_func(\n",
    "                aggregate_columns,\n",
    "                base_image=\"python:3.7\",\n",
    "                packages_to_install=['pandas'])\n",
    "\n",
    "\n",
    "compress_json_to_gzip_op= comp.create_component_from_func(\n",
    "            compress_json_to_gzip,\n",
    "            base_image=\"python:3.7\",\n",
    "            packages_to_install=['pandas', 'gzip', 'json'])\n",
    "\n",
    "compress_csv_to_gzip_op= comp.create_component_from_func(\n",
    "            compress_csv_to_gzip,\n",
    "            base_image=\"python:3.7\",\n",
    "            packages_to_install=['pandas', 'gzip'])"
   ],
   "id": "e94b7183597f1a9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#create pipeline components based on pipeline config object\n",
    "\n",
    "def create_component_for_stage(stage: 'Stage') -> comp.ComponentOp:\n",
    "\n",
    "    if stage.type == 'filtering':\n",
    "      return comp.create_component_from_func(\n",
    "            filtering,\n",
    "            base_image=\"python:3.7\",\n",
    "            packages_to_install=['pandas'])\n",
    "\n",
    "    elif stage.type == 'anonymization':\n",
    "        return comp.create_component_from_func(\n",
    "                        anonymize_columns,\n",
    "                        base_image=\"python:3.7\",\n",
    "                        packages_to_install=['pandas'])\n",
    "\n",
    "    elif stage.type == 'aggregation':\n",
    "       return comp.create_component_from_func(\n",
    "                aggregate_columns,\n",
    "                base_image=\"python:3.7\",\n",
    "                packages_to_install=['pandas'])\n",
    "\n",
    "    elif stage.type == 'compress_json_to_gzip':\n",
    "       return comp.create_component_from_func(\n",
    "            compress_json_to_gzip,\n",
    "            base_image=\"python:3.7\",\n",
    "            packages_to_install=['pandas', 'gzip', 'json'])\n",
    "\n",
    "    elif stage.type == 'compress_csv_to_gzip':\n",
    "        return comp.create_component_from_func(\n",
    "            compress_csv_to_gzip,\n",
    "            base_image=\"python:3.7\",\n",
    "            packages_to_install=['pandas', 'gzip'])\n",
    "    else:\n",
    "        print(f\"Error: Unknown stage type '{stage.type}'\")\n",
    "        return None  \n",
    "\n",
    "\n",
    "# def create_component_for_stage(stage: 'Stage'):\n",
    "#     if stage.type == 'filtering':\n",
    "#         return filtering\n",
    "#     elif stage.type == 'anonymization':\n",
    "#         return anonymize_columns\n",
    "#     elif stage.type == 'aggregation':\n",
    "#         return aggregate_columns\n",
    "#     elif stage.type == 'compress_json_to_gzip':\n",
    "#         return compress_json_to_gzip\n",
    "#     elif stage.type == 'compress_csv_to_gzip':\n",
    "#         return compress_csv_to_gzip\n",
    "#     else:\n",
    "#         print(f\"Error: Unknown stage type '{stage.type}'\")\n",
    "#         return None\n"
   ],
   "id": "2cd42725ed68ae26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# creating the pipelines based on the pipelines config file\n",
    "\n",
    "def dynamic_pipeline(df: pd.DataFrame, pipeline_config: PipelineConfig , pipeline:Pipeline):\n",
    "    # Process each pipeline stage dynamically\n",
    "    data_op = df\n",
    "    \n",
    "    for stage_name in pipeline.flow:\n",
    "            # Find the stage by name\n",
    "            stage = next((s for s in pipeline_config.stages if s.name == stage_name), None)\n",
    "\n",
    "            if stage is None:\n",
    "                print(f\"Error: Stage {stage_name} not found in the configuration.\")\n",
    "                continue\n",
    "\n",
    "            # Create component for the current stage\n",
    "            component_op = create_component_for_stage(stage)\n",
    "\n",
    "            # Dynamically handle each stage\n",
    "            if component_op:\n",
    "                # Depending on the type of stage, pass necessary parameters dynamically\n",
    "                if stage.type == 'filtering':\n",
    "                    data_op = component_op(\n",
    "                        data=data_op, \n",
    "                        operation=stage.parameter['operation'], \n",
    "                        column_name=stage.parameter['column_name'], \n",
    "                        threshold=stage.parameter['threshold']\n",
    "                    )\n",
    "\n",
    "                elif stage.type == 'anonymization':\n",
    "                    data_op = component_op(\n",
    "                        data=data_op, \n",
    "                        columns_to_anonymize=stage.parameter['columns_to_anonymize']\n",
    "                    )\n",
    "\n",
    "                elif stage.type == 'aggregation':\n",
    "                    data_op = component_op(\n",
    "                        data=data_op, \n",
    "                        groupby_column=stage.parameter['groupby_column'], \n",
    "                        columns_to_aggregate=stage.parameter['columns_to_aggregate'], \n",
    "                        aggregation_functions=stage.parameter['aggregation_functions']\n",
    "                    )\n",
    "\n",
    "                elif stage.type == 'compress_json_to_gzip':\n",
    "                    data_op = component_op(\n",
    "                        data=data_op, \n",
    "                        output_file=stage.parameter['output_file']\n",
    "                    )\n",
    "\n",
    "                elif stage.type == 'compress_csv_to_gzip':\n",
    "                    data_op = component_op(\n",
    "                        data=data_op, \n",
    "                        output_file=stage.parameter['output_file']\n",
    "                    )\n",
    "            else:\n",
    "                raise Exception(f\"Component for stage '{stage_name}' could not be created.\")\n",
    "\n",
    "    return data_op\n",
    "\n",
    "\n"
   ],
   "id": "481a87eb74e1592b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TODO: (first) pipeline datasource.type must be csv and it's path.\n",
    "#TODO: support pipelines chains (one pipeline call the other)\n",
    "\n",
    "def create_pipeline_for_each_config(pipeline_config: PipelineConfig, client: kfp.Client):\n",
    "    df = None  # Placeholder for your initial dataset, for example from a CSV\n",
    "    # Iterate over every pipeline in the pipeline config\n",
    "    for pipeline in pipeline_config.pipelines:\n",
    "        # Generate a unique name for each pipeline\n",
    "        pipeline_name = f\"pipeline_{pipeline.name}\"\n",
    "\n",
    "        # Define a Kubeflow pipeline dynamically for each pipeline\n",
    "        @dsl.pipeline(\n",
    "            name=pipeline_name,\n",
    "            description=f\"Pipeline generated from config: {pipeline.name}\"\n",
    "        )\n",
    "        def kubeflow_pipeline():\n",
    "            # Call dynamic_pipeline to handle the stages of this specific pipeline\n",
    "            dynamic_pipeline(df, pipeline_config, pipeline)\n",
    "\n",
    "        # Compile the pipeline\n",
    "        pipeline_file_name = pipeline_name + '.zip'\n",
    "        kfp.compiler.Compiler().compile(kubeflow_pipeline, pipeline_file_name)\n",
    "        print(f\"Pipeline '{pipeline_name}' compiled successfully.\")\n",
    "        \n",
    "        # Upload the compiled pipeline to the specified namespace\n",
    "        client.upload_pipeline(pipeline_file_name, name=pipeline_name)\n",
    "        print(f\"Pipeline '{pipeline_name}' uploaded successfully.\")\n",
    "\n",
    "    return \"All pipelines compiled successfully!\""
   ],
   "id": "fad906899a0dd1ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \n",
    "    # Load the pipeline configuration from a YAML file\n",
    "    pipeline_config_file = 'data\\pipeline_config_sample.yaml'  # Make sure this file exists in your working directory\n",
    "    pipeline_config = read_yaml_to_pipeline_config(pipeline_config_file)\n",
    "    \n",
    "    if pipeline_config.deployment.namespace is None:\n",
    "        raise Exception('please specify the kubeflow namespace in the yml file.')\n",
    "    \n",
    "    client = kfp.Client(namespace=pipeline_config.deployment.namespace)\n",
    "    # Create and compile pipelines based on the configuration\n",
    "    result = create_pipeline_for_each_config(pipeline_config, client)\n",
    "    print(result)  # Optionally print the result message\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "6836af954fe9b847",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9a6c625e97d827dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
