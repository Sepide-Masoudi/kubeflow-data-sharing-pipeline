{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is KubeFlow pipeline Auto Generator\n",
    "\n",
    "Below is the implementation of a pipeline autogenerate based on a config.yaml file. "
   ],
   "id": "1f53509921cbff1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T15:52:00.734833Z",
     "start_time": "2025-03-16T15:52:00.731214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#from numpy import number\n",
    "#!pip install kfp pandas pyyaml\n",
    "#!pip install --upgrade kfp\n",
    "#!pip install kfp==1.8.22 pandas pyyaml"
   ],
   "id": "c4a3b2481440aa50",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T15:52:00.748187Z",
     "start_time": "2025-03-16T15:52:00.745253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gzip\n",
    "from dbm import error\n",
    "\n",
    "import kfp\n",
    "import pandas as pd\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "import hashlib\n",
    "from typing import List, Dict, Any\n",
    "from kfp.dsl import Input, Output, Dataset\n",
    "import yaml\n"
   ],
   "id": "e3159b34d7c68f76",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T15:52:00.773765Z",
     "start_time": "2025-03-16T15:52:00.759187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# stages' function definitions\n",
    "# You can add the stages of your interest here, also add the mapping for them to the \"create_component_for_stage()\" as well.\n",
    "\n",
    "# filtering \n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas','requests']\n",
    ")\n",
    "def filtering(data_path:str, operation: str, column_name: str, threshold: int, output_json: dsl.Output[Dataset]):\n",
    "    \n",
    "    import pandas as pd \n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    def compare_rows(data: pd.DataFrame, column_name: str, threshold: int, operation: str) -> pd.DataFrame:\n",
    "        if operation == 'greater_than':\n",
    "            return data[data[column_name] > threshold]\n",
    "        elif operation == 'less_than':\n",
    "            return data[data[column_name] < threshold]\n",
    "        elif operation == 'equal_to':\n",
    "            return data[data[column_name] == threshold]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported operation\")\n",
    "\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #data =pd.read_csv(data_path)\n",
    "        response = requests.get(data_path)\n",
    "        if response.status_code == 200:\n",
    "            data_json = response.json()  # Parse the JSON data from the response\n",
    "            data = pd.DataFrame(data_json)  # Create a DataFrame from the JSON data\n",
    "        else:\n",
    "            raise ValueError(f\"problem in rendering the URL response from path '{data_path}'\")  \n",
    "        \n",
    "        if column_name not in data.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")  \n",
    "        \n",
    "        filtered_data=compare_rows(data, column_name, threshold, operation)\n",
    "        if filtered_data.empty:\n",
    "            print(\"filtered dat was Empty !!!\")\n",
    "         \n",
    "        with open(output_json.path, 'w') as f:\n",
    "            json.dump(filtered_data.to_dict(orient='records'), f)  \n",
    "            \n",
    "        #filtered_data.to_csv(data_path, index=False)\n",
    "        print(f\"Filtered data has been written to {data_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{data_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "        \n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas']\n",
    ")\n",
    "def anonymize_columns(columns_to_anonymize: list, input_json: dsl.Input[Dataset],output_json: dsl.Output[Dataset]):\n",
    "    #data_path: str,\n",
    "    import pandas as pd \n",
    "    import json\n",
    "    import hashlib\n",
    "    #data_df = pd.read_csv(data_path)\n",
    "    with open(input_json.path, 'r') as f:\n",
    "        data = json.load(f)  \n",
    "    # Check if data is a list of dictionaries\n",
    "    if isinstance(data, list):\n",
    "        data_df = pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        data_df = pd.DataFrame.from_dict(data)\n",
    "    else:\n",
    "        raise ValueError(\"Data is not in a valid format for DataFrame.\")\n",
    "    \n",
    "    for column in columns_to_anonymize:\n",
    "        if column in data_df.columns:\n",
    "            # Anonymize the column using SHA-256 hashing\n",
    "            data_df[column] = data_df[column].apply(lambda x: hashlib.sha256(str(x).encode()).hexdigest())\n",
    "        else:\n",
    "            print(f\"Warning: Column '{column}' not found in the data.\")\n",
    "\n",
    "        with open(output_json.path, 'w') as f:\n",
    "            json.dump(data_df.to_dict(orient='records'), f)  \n",
    "        #data_df.to_csv(data_path, index=False)\n",
    "        \n",
    "# Aggregation\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas','typing']\n",
    ")\n",
    "def aggregate_columns(\n",
    "    groupby_column: str, \n",
    "    columns_to_aggregate: list,  # list[str]\n",
    "    aggregation_functions: dict,  # dict[str, list[str]], \n",
    "    input_json: dsl.Input[Dataset],\n",
    "    output_json: dsl.Output[Dataset]\n",
    "):\n",
    "    import pandas as pd  \n",
    "    import json\n",
    "    \n",
    "    #data_df = pd.read_csv(data_path)\n",
    "    with open(input_json.path, 'r') as f:\n",
    "        data = json.load(f) \n",
    "    # Check if data is a list of dictionaries\n",
    "    if isinstance(data, list):\n",
    "        data_df = pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        data_df = pd.DataFrame.from_dict(data)\n",
    "    else:\n",
    "        raise ValueError(\"Data is not in a valid format for DataFrame.\")\n",
    "    \n",
    "    try:\n",
    "        # Perform the aggregation\n",
    "        aggregated_data = data_df.groupby(groupby_column).agg(aggregation_functions)\n",
    "         # Flatten multi-index columns if they exist\n",
    "        if isinstance(aggregated_data.columns, pd.MultiIndex):\n",
    "            aggregated_data.columns = ['_'.join(col).strip() for col in aggregated_data.columns.values]\n",
    "\n",
    "        # Select only the required columns to aggregate\n",
    "        #aggregated_data = aggregated_data[columns_to_aggregate]\n",
    "        # Check if all columns to aggregate are in the result, then select them\n",
    "        available_columns = [col for col in columns_to_aggregate if col in aggregated_data.columns]\n",
    "        aggregated_data = aggregated_data[available_columns]\n",
    "        aggregated_data.reset_index(inplace=True)\n",
    "        # Save the result to the output path\n",
    "        #aggregated_data.to_csv(data_path, index=False)\n",
    "        print(f\"Aggregated data has been written to {aggregated_data.to_dict(orient='records')}\")\n",
    "        with open(output_json.path, 'w') as f:\n",
    "            json.dump(aggregated_data.to_dict(orient='records'), f)          \n",
    "    except KeyError as e:\n",
    "        raise f\"Error: Column {e} not found in the data.\"\n",
    "    except Exception as e:\n",
    "        raise f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "# Function to compress JSON to GZIP\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas']\n",
    ")\n",
    "def compress_json_to_gzip(\n",
    "    input_json: dsl.Input[Dataset],\n",
    "    output_json: dsl.Output[Dataset]\n",
    "):\n",
    "    import json\n",
    "    import gzip\n",
    "\n",
    "    # Read JSON data from input file\n",
    "    with open(input_json.path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    try:\n",
    "        # Convert data to JSON string\n",
    "        json_data = json.dumps(data)\n",
    "        \n",
    "        # Compress and write to GZIP file\n",
    "        with gzip.open(output_json.path, 'wt', encoding='utf-8') as f:\n",
    "            f.write(json_data)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Optionally save an empty file in case of failure\n",
    "        with open(output_json.path, 'w') as f:\n",
    "            f.write('')\n",
    "            \n",
    "# Function to compress CSV to GZIP\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas']    \n",
    ")\n",
    "def compress_csv_to_gzip(\n",
    "    input_json: dsl.Input[Dataset],\n",
    "    output_json: dsl.Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import gzip\n",
    "\n",
    "    # Convert Dataset to DataFrame\n",
    "    with open(input_json.path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        data_df = pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        data_df = pd.DataFrame.from_dict(data)\n",
    "    else:\n",
    "        raise ValueError(\"Data is not in a valid format for DataFrame.\")\n",
    "\n",
    "    # Compress DataFrame to CSV GZIP\n",
    "    try:\n",
    "        with gzip.open(output_json.path, 'wt') as f:\n",
    "            data_df.to_csv(f, index=False)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred during compression: {e}\")"
   ],
   "id": "49f531cb6310a64b",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T15:52:00.780529Z",
     "start_time": "2025-03-16T15:52:00.777070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# parse the yaml file to create the kubeflow pipeline:\n",
    "\n",
    "# Deployment of the kubeflow\n",
    "class Deployment:\n",
    "    def __init__(self, namespace: str, prometheusURL: str ):\n",
    "       self.namespace = namespace\n",
    "       self.prometheusURL = prometheusURL\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'namespace': self.namespace,\n",
    "            'prometheusURL': self.prometheusURL\n",
    "        }        \n",
    "\n",
    "#\n",
    "class Stage:\n",
    "    def __init__(self, name: str, type: str ,parameter: Dict[str, Any] ):\n",
    "        self.name = name\n",
    "        self.type = type\n",
    "        self.parameter = parameter\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'type': self.type,\n",
    "            'parameter': self.parameter\n",
    "        }\n",
    "    \n",
    "# pipeline name must be unique in the whole config file\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, name:str, flow:List[str], consumers:List[str]):\n",
    "        self.flow = flow\n",
    "        self.name = name\n",
    "        self.consumers = consumers\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'flow': self.flow,\n",
    "            'consumers': self.consumers\n",
    "        }\n",
    "    \n",
    "class PipelineChain:\n",
    "    def __init__(self, name:str, flow:List[str]):\n",
    "        self.flow = flow\n",
    "        self.name = name\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'flow': self.flow\n",
    "        }\n",
    "    \n",
    "        \n",
    "class PipelineConfig:\n",
    "    def __init__(self,pipelines:List[Pipeline],stages:List[Stage],deployment: Deployment , pipeline_chains:List[PipelineChain]):\n",
    "        self.pipelines = pipelines\n",
    "        self.stages = stages\n",
    "        self.deployment = deployment\n",
    "        self.pipeline_chains = pipeline_chains\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'deployment': self.deployment.to_dict(),            \n",
    "            'stages': [stage.to_dict() for stage in self.stages],\n",
    "            'pipelines': [pipeline.to_dict() for pipeline in self.pipelines],\n",
    "            'pipelineChains': [chain.to_dict() for chain in self.pipeline_chains]\n",
    "        }        \n",
    "         \n",
    "        "
   ],
   "id": "918821abfd141512",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T15:52:00.786457Z",
     "start_time": "2025-03-16T15:52:00.784357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_component_for_stage(stage: 'Stage'):\n",
    "    if stage.type == 'filtering':\n",
    "        return filtering\n",
    "    elif stage.type == 'anonymization':\n",
    "        return anonymize_columns\n",
    "    elif stage.type == 'aggregation':\n",
    "        return aggregate_columns\n",
    "    elif stage.type == 'compress_json_to_gzip':\n",
    "        return compress_json_to_gzip\n",
    "    elif stage.type == 'compress_csv_to_gzip':\n",
    "        return compress_csv_to_gzip\n",
    "    else:\n",
    "        print(f\"Error: Unknown stage type '{stage.type}'\")\n",
    "        return None"
   ],
   "id": "8f321e72cfc78058",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T15:52:00.793230Z",
     "start_time": "2025-03-16T15:52:00.790252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# creating the pipelines based on the pipelines config file\n",
    "from kubernetes.client import V1Pod, V1ObjectMeta\n",
    "\n",
    "def dynamic_pipeline(data_path: str, pipeline_config: PipelineConfig, pipeline: Pipeline):\n",
    "    # Process each pipeline stage dynamically\n",
    "    global output_data\n",
    "    \n",
    "    for stage_name in pipeline.flow:\n",
    "        # Find the stage by name\n",
    "        stage = next((s for s in pipeline_config.stages if s.name == stage_name), None)\n",
    "\n",
    "        if stage is None:\n",
    "            print(f\"Error: Stage {stage_name} not found in the configuration.\")\n",
    "            continue\n",
    "\n",
    "        # Create component for the current stage\n",
    "        component_op = create_component_for_stage(stage)\n",
    "        task_name = f\"{pipeline.name}-{stage.type}-{stage_name}\".replace('_', '-')\n",
    "        # Dynamically handle each stage\n",
    "        if stage.type == 'filtering':\n",
    "           \n",
    "                # Ensure to pass the output as an Output type\n",
    "                output_data=component_op(\n",
    "                    data_path=data_path, \n",
    "                    operation=stage.parameter['operation'], \n",
    "                    column_name=stage.parameter['column_name'], \n",
    "                    threshold=stage.parameter['threshold']\n",
    "                )\n",
    "                #output_data.set_display_name(task_name)\n",
    "                #output_data.pod_overrides = V1Pod(metadata=V1ObjectMeta(labels={\"custom_name\": task_name}))\n",
    "  \n",
    "        elif stage.type == 'anonymization':\n",
    "                 output_data=component_op(\n",
    "                    input_json=output_data.outputs['output_json'], \n",
    "                    columns_to_anonymize=stage.parameter['columns_to_anonymize']\n",
    "                 )\n",
    "                 #output_data.set_display_name(task_name)\n",
    "                 \n",
    "        \n",
    "        elif stage.type == 'aggregation':\n",
    "               output_data=component_op(\n",
    "                    input_json=output_data.outputs['output_json'], \n",
    "                    groupby_column=stage.parameter['groupby_column'], \n",
    "                    columns_to_aggregate=stage.parameter['columns_to_aggregate'], \n",
    "                    aggregation_functions=stage.parameter['aggregation_functions']\n",
    "        )\n",
    "               #output_data.set_display_name(task_name)\n",
    "                \n",
    "        elif stage.type == 'compress_json_to_gzip':\n",
    "               output_data=component_op(\n",
    "                 input_json =output_data.outputs['output_json']\n",
    "        )\n",
    "               #output_data.set_display_name(task_name)\n",
    "\n",
    "        elif stage.type == 'compress_csv_to_gzip':\n",
    "                output_data=component_op(\n",
    "                    input_json=output_data.outputs['output_json']\n",
    "        )\n",
    "                #output_data.set_display_name(task_name)\n",
    "        else:\n",
    "            raise Exception(f\"Component for stage '{stage_name}' could not be created.\")\n",
    "\n",
    "    return output_data\n",
    "\n",
    "\n"
   ],
   "id": "de71969508f949a4",
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T15:52:00.804805Z",
     "start_time": "2025-03-16T15:52:00.802309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to read YAML and convert to PipelineConfig\n",
    "def read_yaml_to_pipeline_config(file_path: str) -> PipelineConfig:\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "\n",
    "        # Create Stage objects from the YAML stages\n",
    "        stages = [Stage(name=stage['name'], type=stage['type'], parameter=stage['parameter']) for stage in data['stages']]\n",
    "\n",
    "        # Create Pipeline objects from the YAML pipelines\n",
    "        pipelines = []\n",
    "        for pipeline in data['pipelines']:\n",
    "            \n",
    "            pipelines.append(Pipeline(name=pipeline['name'], flow=pipeline['flow'], consumers=pipeline['consumers']))\n",
    "\n",
    "        deployment_data = data.get('Deployment', {})\n",
    "        deployment = Deployment(namespace=deployment_data.get('namespace', ''),\n",
    "                                prometheusURL=deployment_data.get('prometheusURL', ''))\n",
    "        \n",
    "        pipeline_chains = []\n",
    "        if 'pipelineChains' in data:\n",
    "            for pipelineChain in data['pipelineChains']:\n",
    "                pipeline_chains.append(PipelineChain(name=pipelineChain['name'], flow=pipelineChain['flow']))\n",
    "        \n",
    "        # Create PipelineConfig object\n",
    "        pipeline_config = PipelineConfig(pipelines=pipelines, stages=stages, deployment=deployment, pipeline_chains=pipeline_chains)\n",
    "\n",
    "        return pipeline_config"
   ],
   "id": "8fb615218b2f5ad3",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T15:52:00.815082Z",
     "start_time": "2025-03-16T15:52:00.813047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to dynamically handle pipeline chains\n",
    "def dynamic_pipeline_chain(data_path: str, pipeline_config: PipelineConfig, chain: PipelineChain):\n",
    "\n",
    "    final_output = None\n",
    "    # Iterate through each pipeline in the chain's flow\n",
    "    for pipeline_name in chain.flow:\n",
    "        # Find the pipeline by name\n",
    "        pipeline = next((p for p in pipeline_config.pipelines if p.name == pipeline_name), None)\n",
    "        if pipeline is None:\n",
    "            print(f\"Error: Pipeline {pipeline_name} not found in the configuration.\")\n",
    "            continue\n",
    "        \n",
    "        # Call dynamic_pipeline for each pipeline in the chain\n",
    "        final_output = dynamic_pipeline(data_path, pipeline_config, pipeline)\n",
    "    \n",
    "    return final_output"
   ],
   "id": "f390a1b475872e81",
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T15:52:00.821978Z",
     "start_time": "2025-03-16T15:52:00.819382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TODO: (first) pipeline datasource.type must be csv and it's path.\n",
    "\n",
    "# Adjusted pipeline creation for chains\n",
    "def create_pipeline_for_each_chain(client: kfp.Client):\n",
    "    ### PARAM_1 : REPLACE WITH YOUR CONFIGURATION FILE PATH\n",
    "    pipeline_config_file = './data/shipments_pipeline_config.yaml'\n",
    "    pipeline_config = read_yaml_to_pipeline_config(pipeline_config_file)\n",
    "    \n",
    "    # Iterate over each pipeline chain in the config\n",
    "    for chain in pipeline_config.pipeline_chains:\n",
    "        # Generate a unique name for each chain\n",
    "        chain_name = f\"chain_{chain.name}\"\n",
    "\n",
    "        # Define a Kubeflow pipeline for each chain\n",
    "        @dsl.pipeline(\n",
    "            name=chain_name,\n",
    "            description=f\"Pipeline chain generated from config: {chain.name}\"\n",
    "        )\n",
    "        def kubeflow_pipeline():\n",
    "            # Call dynamic_pipeline_chain to handle the chain of pipelines\n",
    "            ### PARAM_2 : REPLACE WITH YOUR DATA SET URL\n",
    "            dynamic_pipeline_chain('http://industry.teadal.ubiwhere.com/fdp-czech-plant/shipments', pipeline_config, chain)\n",
    "\n",
    "        # Compile the pipeline chain\n",
    "        chain_file_name = chain_name + '.yaml'\n",
    "        kfp.compiler.Compiler().compile(pipeline_func=kubeflow_pipeline, package_path=chain_file_name)\n",
    "        print(f\"Pipeline chain '{chain_name}' compiled successfully.\")\n",
    "\n",
    "        # Upload the compiled chain to the specified namespace\n",
    "        client.upload_pipeline(pipeline_package_path=chain_file_name, pipeline_name=chain_name)\n",
    "        print(f\"Pipeline chain '{chain_name}' uploaded successfully.\")\n",
    "\n",
    "    return \"All pipeline chains compiled successfully!\""
   ],
   "id": "f00f5d4bbdb782ea",
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T15:52:00.921917Z",
     "start_time": "2025-03-16T15:52:00.826713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "def main():\n",
    "    \n",
    "    client = kfp.Client() \n",
    "    # Create and compile pipelines based on the configuration\n",
    "    result = create_pipeline_for_each_chain(client)\n",
    "    print(result) \n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "d044622fbaac9b78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline chain 'chain_shipments_anonymize_chain' compiled successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<a href=\"/pipeline/#/pipelines/details/69f39504-90b4-4efa-9cc2-e236435c6f6e\" target=\"_blank\" >Pipeline details</a>."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline chain 'chain_shipments_anonymize_chain' uploaded successfully.\n",
      "Pipeline chain 'chain_shipments_data_chain' compiled successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<a href=\"/pipeline/#/pipelines/details/c81c0821-a957-43b2-a1e1-430942a29702\" target=\"_blank\" >Pipeline details</a>."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline chain 'chain_shipments_data_chain' uploaded successfully.\n",
      "All pipeline chains compiled successfully!\n"
     ]
    }
   ],
   "execution_count": 149
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
