{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is KubeFlow pipeline Auto Generator\n",
    "\n",
    "Below is the implementation of a pipeline autogenerate based on a config.yaml file. "
   ],
   "id": "1f53509921cbff1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:04.351244Z",
     "start_time": "2024-12-13T13:16:04.347958Z"
    }
   },
   "cell_type": "code",
   "source": [
    " ########### TODO s :\n",
    " # TODO: fix-bug : ERROR: Could not find a version that satisfies the requirement gzip (from versions: none)ERROR: No matching distribution found for gzip\n",
    " #TODO : Persistent Volume for pipelines (https://stackoverflow.com/questions/68206160/access-persistent-volume-in-kubeflow-from-different-components or IBM solution)\n",
    " # https://github.com/Ark-kun/kfp_samples/blob/65a98da/2019-10%20Kubeflow%20summit/104%20-%20Passing%20data%20for%20python%20components/104%20-%20Passing%20data%20for%20python%20components.ipynb\n",
    " # https://www.kubeflow.org/docs/components/pipelines/user-guides/core-functions/platform-specific-features/\n",
    " # https://www.kubeflow.org/docs/components/pipelines/legacy-v1/sdk/manipulate-resources/\n",
    "#TODO : creating stages from a docker image\n",
    "#TODO : creating pipeline chains\n",
    "#TODO : creating conditional pipelines and pipeline chains\n",
    "#TODO : kepler and Prometheus queries\n",
    "#TODO : filtering stage only cover number-base threshold right now\n",
    "\n",
    "\n",
    "###########TODO : outputs artifacts !!!!!!!!!!!!!!!!\n",
    "######### 1. for experiment (main track paper)\n",
    "#########2. as a tool for general use (poster paper)"
   ],
   "id": "b1125dd488c56fca",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:04.356360Z",
     "start_time": "2024-12-13T13:16:04.354922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#kubectl port-forward -n kepler svc/kepler 9102:9102\n",
    "# kubectl port-forward svc/minio-service 9000:9000 -n kubeflow\n",
    "#kubectl port-forward csv-reader-744cc4b588-mqc5p   5000:5000 -n kubeflow\n",
    "#kubectl port-forward -n monitoring svc/prometheus 9090:9090\n"
   ],
   "id": "acf6c8893af1e5e8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:04.378848Z",
     "start_time": "2024-12-13T13:16:04.376643Z"
    }
   },
   "source": "#https://www.kubeflow.org/docs/components/pipelines/legacy-v1/installation/localcluster-deployment/",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:04.980467Z",
     "start_time": "2024-12-13T13:16:04.384446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gzip\n",
    "from dbm import error\n",
    "\n",
    "import kfp\n",
    "import pandas as pd\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "import hashlib\n",
    "from typing import List, Dict, Any\n",
    "from kfp.dsl import Input, Output, Dataset\n",
    "import yaml\n",
    "\n",
    "\n",
    "# @dsl.component(base_image='python:3.12.2')\n",
    "# def say_hello(input_path:str,output_file:str):\n",
    "#     data =pd.read_csv(input_path)\n",
    "#     if data is not None:\n",
    "#             json_data = data.to_json(orient='records')  # Convert DataFrame to JSON\n",
    "#             with gzip.open(output_file, 'wt', encoding='utf-8') as f:\n",
    "#                 f.write(json_data)\n",
    "#             return output_file "
   ],
   "id": "e3159b34d7c68f76",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:04.998928Z",
     "start_time": "2024-12-13T13:16:04.988893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# stages' function definitions\n",
    "\n",
    "# filtering\n",
    "\n",
    "    \n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas','requests']\n",
    ")\n",
    "def filtering(data_path:str, operation: str, column_name: str, threshold: int, output_json: dsl.Output[Dataset]):\n",
    "    \n",
    "    import pandas as pd \n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    def compare_rows(data: pd.DataFrame, column_name: str, threshold: int, operation: str) -> pd.DataFrame:\n",
    "        if operation == 'greater_than':\n",
    "            return data[data[column_name] > threshold]\n",
    "        elif operation == 'less_than':\n",
    "            return data[data[column_name] < threshold]\n",
    "        elif operation == 'equal_to':\n",
    "            return data[data[column_name] == threshold]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported operation\")\n",
    "\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #data =pd.read_csv(data_path)\n",
    "        response = requests.get(data_path)\n",
    "        if response.status_code == 200:\n",
    "            data_json = response.json()  # Parse the JSON data from the response\n",
    "            data = pd.DataFrame(data_json)  # Create a DataFrame from the JSON data\n",
    "        else:\n",
    "            raise ValueError(f\"problem in rendering the URL response from path '{data_path}'\")  \n",
    "        \n",
    "        if column_name not in data.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")  \n",
    "        \n",
    "        filtered_data=compare_rows(data, column_name, threshold, operation)\n",
    "        if filtered_data.empty:\n",
    "            print(\"filtered dat was Empty !!!\")\n",
    "         \n",
    "        with open(output_json.path, 'w') as f:\n",
    "            json.dump(filtered_data.to_dict(orient='records'), f)  \n",
    "            \n",
    "        #filtered_data.to_csv(data_path, index=False)\n",
    "        print(f\"Filtered data has been written to {data_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{data_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "        \n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas']\n",
    ")\n",
    "def anonymize_columns(columns_to_anonymize: list, input_json: dsl.Input[Dataset],output_json: dsl.Output[Dataset]):\n",
    "    #data_path: str,\n",
    "    import pandas as pd \n",
    "    import json\n",
    "    import hashlib\n",
    "    #data_df = pd.read_csv(data_path)\n",
    "    with open(input_json.path, 'r') as f:\n",
    "        data = json.load(f)  \n",
    "    # Check if data is a list of dictionaries\n",
    "    if isinstance(data, list):\n",
    "        data_df = pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        data_df = pd.DataFrame.from_dict(data)\n",
    "    else:\n",
    "        raise ValueError(\"Data is not in a valid format for DataFrame.\")\n",
    "    \n",
    "    for column in columns_to_anonymize:\n",
    "        if column in data_df.columns:\n",
    "            # Anonymize the column using SHA-256 hashing\n",
    "            data_df[column] = data_df[column].apply(lambda x: hashlib.sha256(str(x).encode()).hexdigest())\n",
    "        else:\n",
    "            print(f\"Warning: Column '{column}' not found in the data.\")\n",
    "\n",
    "        with open(output_json.path, 'w') as f:\n",
    "            json.dump(data_df.to_dict(orient='records'), f)  \n",
    "        #data_df.to_csv(data_path, index=False)\n",
    "        \n",
    "# Aggregation\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas','typing']\n",
    ")\n",
    "def aggregate_columns(\n",
    "    groupby_column: str, \n",
    "    columns_to_aggregate: list,  # list[str]\n",
    "    aggregation_functions: dict,  # dict[str, list[str]], \n",
    "    input_json: dsl.Input[Dataset],\n",
    "    output_json: dsl.Output[Dataset]\n",
    "):\n",
    "    import pandas as pd  \n",
    "    import json\n",
    "    \n",
    "    #data_df = pd.read_csv(data_path)\n",
    "    with open(input_json.path, 'r') as f:\n",
    "        data = json.load(f) \n",
    "    # Check if data is a list of dictionaries\n",
    "    if isinstance(data, list):\n",
    "        data_df = pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        data_df = pd.DataFrame.from_dict(data)\n",
    "    else:\n",
    "        raise ValueError(\"Data is not in a valid format for DataFrame.\")\n",
    "    \n",
    "    try:\n",
    "        # Perform the aggregation\n",
    "        aggregated_data = data_df.groupby(groupby_column).agg(aggregation_functions)\n",
    "         # Flatten multi-index columns if they exist\n",
    "        if isinstance(aggregated_data.columns, pd.MultiIndex):\n",
    "            aggregated_data.columns = ['_'.join(col).strip() for col in aggregated_data.columns.values]\n",
    "\n",
    "        # Select only the required columns to aggregate\n",
    "        #aggregated_data = aggregated_data[columns_to_aggregate]\n",
    "        # Check if all columns to aggregate are in the result, then select them\n",
    "        available_columns = [col for col in columns_to_aggregate if col in aggregated_data.columns]\n",
    "        aggregated_data = aggregated_data[available_columns]\n",
    "        aggregated_data.reset_index(inplace=True)\n",
    "        # Save the result to the output path\n",
    "        #aggregated_data.to_csv(data_path, index=False)\n",
    "        print(f\"Aggregated data has been written to {aggregated_data.to_dict(orient='records')}\")\n",
    "        with open(output_json.path, 'w') as f:\n",
    "            json.dump(aggregated_data.to_dict(orient='records'), f)          \n",
    "    except KeyError as e:\n",
    "        raise f\"Error: Column {e} not found in the data.\"\n",
    "    except Exception as e:\n",
    "        raise f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "# Function to compress JSON to GZIP\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas']\n",
    ")\n",
    "def compress_json_to_gzip(\n",
    "    input_json: dsl.Input[Dataset],\n",
    "    output_json: dsl.Output[Dataset]\n",
    "):\n",
    "    import json\n",
    "    import gzip\n",
    "\n",
    "    # Read JSON data from input file\n",
    "    with open(input_json.path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    try:\n",
    "        # Convert data to JSON string\n",
    "        json_data = json.dumps(data)\n",
    "        \n",
    "        # Compress and write to GZIP file\n",
    "        with gzip.open(output_json.path, 'wt', encoding='utf-8') as f:\n",
    "            f.write(json_data)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Optionally save an empty file in case of failure\n",
    "        with open(output_json.path, 'w') as f:\n",
    "            f.write('')\n",
    "            \n",
    "# Function to compress CSV to GZIP\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.12.2\",\n",
    "    packages_to_install=['pandas']\n",
    ")\n",
    "def compress_csv_to_gzip(\n",
    "    input_json: dsl.Input[Dataset],\n",
    "    output_json: dsl.Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import gzip\n",
    "\n",
    "    # Convert Dataset to DataFrame\n",
    "    with open(input_json.path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        data_df = pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        data_df = pd.DataFrame.from_dict(data)\n",
    "    else:\n",
    "        raise ValueError(\"Data is not in a valid format for DataFrame.\")\n",
    "\n",
    "    # Compress DataFrame to CSV GZIP\n",
    "    try:\n",
    "        with gzip.open(output_json.path, 'wt') as f:\n",
    "            data_df.to_csv(f, index=False)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred during compression: {e}\")"
   ],
   "id": "49f531cb6310a64b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:05.005664Z",
     "start_time": "2024-12-13T13:16:05.001383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# parse the yaml file to create the kubeflow pipeline:\n",
    "\n",
    "# Deployment of the kubeflow\n",
    "class Deployment:\n",
    "    def __init__(self, namespace: str, prometheusURL: str ):\n",
    "       self.namespace = namespace\n",
    "       self.prometheusURL = prometheusURL\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'namespace': self.namespace,\n",
    "            'prometheusURL': self.prometheusURL\n",
    "        }        \n",
    "\n",
    "#\n",
    "class Stage:\n",
    "    def __init__(self, name: str, type: str ,parameter: Dict[str, Any] ):\n",
    "        self.name = name\n",
    "        self.type = type\n",
    "        self.parameter = parameter\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'type': self.type,\n",
    "            'parameter': self.parameter\n",
    "        }\n",
    "    \n",
    "#type : csv , pipeline\n",
    "#metadat = filepath or next name of pipeline to call.          \n",
    "class Datasource:\n",
    "    def __init__(self, type: str, metadata: str):\n",
    "        self.type = type\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'type': self.type,\n",
    "            'metadata': self.metadata\n",
    "        }\n",
    "# pipeline name must be unique in the whole config file\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, name:str, flow:List[str], datasource:Datasource , consumers:List[str]):\n",
    "        self.flow = flow\n",
    "        self.datasource = datasource\n",
    "        self.name = name\n",
    "        self.consumers = consumers\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'flow': self.flow,\n",
    "            'datasource': self.datasource.to_dict(),\n",
    "            'consumers': self.consumers\n",
    "        }\n",
    "    \n",
    "class PipelineChain:\n",
    "    def __init__(self, name:str, flow:List[str]):\n",
    "        self.flow = flow\n",
    "        self.name = name\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'flow': self.flow\n",
    "        }\n",
    "    \n",
    "        \n",
    "class PipelineConfig:\n",
    "    def __init__(self,pipelines:List[Pipeline],stages:List[Stage],deployment: Deployment , pipeline_chains:List[PipelineChain]):\n",
    "        self.pipelines = pipelines\n",
    "        self.stages = stages\n",
    "        self.deployment = deployment\n",
    "        self.pipeline_chains = pipeline_chains\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'deployment': self.deployment.to_dict(),            \n",
    "            'stages': [stage.to_dict() for stage in self.stages],\n",
    "            'pipelines': [pipeline.to_dict() for pipeline in self.pipelines],\n",
    "            'pipelineChains': [chain.to_dict() for chain in self.pipeline_chains]\n",
    "        }        \n",
    "         \n",
    "        "
   ],
   "id": "918821abfd141512",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:05.011388Z",
     "start_time": "2024-12-13T13:16:05.009538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_component_for_stage(stage: 'Stage'):\n",
    "    if stage.type == 'filtering':\n",
    "        return filtering\n",
    "    elif stage.type == 'anonymization':\n",
    "        return anonymize_columns\n",
    "    elif stage.type == 'aggregation':\n",
    "        return aggregate_columns\n",
    "    elif stage.type == 'compress_json_to_gzip':\n",
    "        return compress_json_to_gzip\n",
    "    elif stage.type == 'compress_csv_to_gzip':\n",
    "        return compress_csv_to_gzip\n",
    "    else:\n",
    "        print(f\"Error: Unknown stage type '{stage.type}'\")\n",
    "        return None"
   ],
   "id": "8f321e72cfc78058",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:05.017712Z",
     "start_time": "2024-12-13T13:16:05.015038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# creating the pipelines based on the pipelines config file\n",
    "\n",
    "def dynamic_pipeline(data_path: str, pipeline_config: PipelineConfig, pipeline: Pipeline):\n",
    "    # Process each pipeline stage dynamically\n",
    "    global output_data\n",
    "    \n",
    "    for stage_name in pipeline.flow:\n",
    "        # Find the stage by name\n",
    "        stage = next((s for s in pipeline_config.stages if s.name == stage_name), None)\n",
    "\n",
    "        if stage is None:\n",
    "            print(f\"Error: Stage {stage_name} not found in the configuration.\")\n",
    "            continue\n",
    "\n",
    "        # Create component for the current stage\n",
    "        component_op = create_component_for_stage(stage)\n",
    "        # Dynamically handle each stage\n",
    "        if stage.type == 'filtering':\n",
    "           \n",
    "                # Ensure to pass the output as an Output type\n",
    "                output_data=component_op(\n",
    "                    data_path=data_path, \n",
    "                    operation=stage.parameter['operation'], \n",
    "                    column_name=stage.parameter['column_name'], \n",
    "                    threshold=stage.parameter['threshold']\n",
    "                )\n",
    "        \n",
    "        elif stage.type == 'anonymization':\n",
    "                 output_data=component_op(\n",
    "                    input_json=output_data.outputs['output_json'], \n",
    "                    columns_to_anonymize=stage.parameter['columns_to_anonymize']\n",
    "                 )\n",
    "        \n",
    "        elif stage.type == 'aggregation':\n",
    "               output_data=component_op(\n",
    "                    input_json=output_data.outputs['output_json'], \n",
    "                    groupby_column=stage.parameter['groupby_column'], \n",
    "                    columns_to_aggregate=stage.parameter['columns_to_aggregate'], \n",
    "                    aggregation_functions=stage.parameter['aggregation_functions']\n",
    "        )\n",
    "                \n",
    "        elif stage.type == 'compress_json_to_gzip':\n",
    "               output_data=component_op(\n",
    "                 input_json =output_data.outputs['output_json']\n",
    "        )\n",
    "\n",
    "        elif stage.type == 'compress_csv_to_gzip':\n",
    "                output_data=component_op(\n",
    "                    input_json=output_data.outputs['output_json']\n",
    "        )\n",
    "        else:\n",
    "            raise Exception(f\"Component for stage '{stage_name}' could not be created.\")\n",
    "\n",
    "    return output_data\n",
    "\n",
    "\n"
   ],
   "id": "de71969508f949a4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:05.023797Z",
     "start_time": "2024-12-13T13:16:05.021475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to read YAML and convert to PipelineConfig\n",
    "def read_yaml_to_pipeline_config(file_path: str) -> PipelineConfig:\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "\n",
    "        # Create Stage objects from the YAML stages\n",
    "        stages = [Stage(name=stage['name'], type=stage['type'], parameter=stage['parameter']) for stage in data['stages']]\n",
    "\n",
    "        # Create Pipeline objects from the YAML pipelines\n",
    "        pipelines = []\n",
    "        for pipeline in data['pipelines']:\n",
    "            datasource = Datasource(type=pipeline['datasource']['type'], metadata=pipeline['datasource']['metadata'])\n",
    "            pipelines.append(Pipeline(name=pipeline['name'], flow=pipeline['flow'], datasource=datasource,consumers=pipeline['consumers']))\n",
    "\n",
    "        deployment_data = data.get('Deployment', {})\n",
    "        deployment = Deployment(namespace=deployment_data.get('namespace', ''),\n",
    "                                prometheusURL=deployment_data.get('prometheusURL', ''))\n",
    "        \n",
    "        pipeline_chains = []\n",
    "        if 'pipelineChains' in data:\n",
    "            for pipelineChain in data['pipelineChains']:\n",
    "                pipeline_chains.append(PipelineChain(name=pipelineChain['name'], flow=pipelineChain['flow']))\n",
    "        \n",
    "        # Create PipelineConfig object\n",
    "        pipeline_config = PipelineConfig(pipelines=pipelines, stages=stages, deployment=deployment, pipeline_chains=pipeline_chains)\n",
    "\n",
    "        return pipeline_config"
   ],
   "id": "8fb615218b2f5ad3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:05.029330Z",
     "start_time": "2024-12-13T13:16:05.027213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to dynamically handle pipeline chains\n",
    "def dynamic_pipeline_chain(data_path: str, pipeline_config: PipelineConfig, chain: PipelineChain):\n",
    "    \"\"\"\n",
    "    This function handles the creation of a dynamic Kubeflow pipeline for a chain of pipelines.\n",
    "    It iterates through the flow of pipelines defined in the chain.\n",
    "    \"\"\"\n",
    "    final_output = None\n",
    "    # Iterate through each pipeline in the chain's flow\n",
    "    for pipeline_name in chain.flow:\n",
    "        # Find the pipeline by name\n",
    "        pipeline = next((p for p in pipeline_config.pipelines if p.name == pipeline_name), None)\n",
    "        if pipeline is None:\n",
    "            print(f\"Error: Pipeline {pipeline_name} not found in the configuration.\")\n",
    "            continue\n",
    "        \n",
    "        # Call dynamic_pipeline for each pipeline in the chain\n",
    "        final_output = dynamic_pipeline(data_path, pipeline_config, pipeline)\n",
    "    \n",
    "    return final_output"
   ],
   "id": "f390a1b475872e81",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:05.034839Z",
     "start_time": "2024-12-13T13:16:05.032617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TODO: (first) pipeline datasource.type must be csv and it's path.\n",
    "\n",
    "# Adjusted pipeline creation for chains\n",
    "def create_pipeline_for_each_chain(client: kfp.Client):\n",
    "    #pipeline_config_file = './data/pipeline_config_sample.yaml'\n",
    "    #pipeline_config_file = './data/new_pipeline/pipeline_config_sample.yaml'\n",
    "    pipeline_config_file = './data/shipments_pipeline_config.yaml'\n",
    "    pipeline_config = read_yaml_to_pipeline_config(pipeline_config_file)\n",
    "    \n",
    "    # Iterate over each pipeline chain in the config\n",
    "    for chain in pipeline_config.pipeline_chains:\n",
    "        # Generate a unique name for each chain\n",
    "        chain_name = f\"chain_{chain.name}\"\n",
    "\n",
    "        # Define a Kubeflow pipeline for each chain\n",
    "        @dsl.pipeline(\n",
    "            name=chain_name,\n",
    "            description=f\"Pipeline chain generated from config: {chain.name}\"\n",
    "        )\n",
    "        def kubeflow_pipeline():\n",
    "            # Call dynamic_pipeline_chain to handle the chain of pipelines\n",
    "            #dynamic_pipeline_chain('http://csv-reader-service.kubeflow.svc.cluster.local:5000/read_csv', pipeline_config, chain)\n",
    "            dynamic_pipeline_chain('http://industry.teadal.ubiwhere.com/fdp-czech-plant/shipments', pipeline_config, chain)\n",
    "\n",
    "        # Compile the pipeline chain\n",
    "        chain_file_name = chain_name + '.yaml'\n",
    "        kfp.compiler.Compiler().compile(pipeline_func=kubeflow_pipeline, package_path=chain_file_name)\n",
    "        print(f\"Pipeline chain '{chain_name}' compiled successfully.\")\n",
    "\n",
    "        # Upload the compiled chain to the specified namespace\n",
    "        client.upload_pipeline(pipeline_package_path=chain_file_name, pipeline_name=chain_name)\n",
    "        print(f\"Pipeline chain '{chain_name}' uploaded successfully.\")\n",
    "\n",
    "    return \"All pipeline chains compiled successfully!\""
   ],
   "id": "f00f5d4bbdb782ea",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:05.140406Z",
     "start_time": "2024-12-13T13:16:05.039020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "def main():\n",
    "    \n",
    "    client = kfp.Client() #namespace=pipeline_config.deployment.namespace\n",
    "    # Create and compile pipelines based on the configuration\n",
    "    result = create_pipeline_for_each_chain(client)\n",
    "    print(result)  # Optionally print the result message\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "d044622fbaac9b78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline chain 'chain_shipments_anonymize_chain' compiled successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<a href=\"/pipeline/#/pipelines/details/7a28e47b-4d41-46df-afa6-4f1bbd06e591\" target=\"_blank\" >Pipeline details</a>."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline chain 'chain_shipments_anonymize_chain' uploaded successfully.\n",
      "Pipeline chain 'chain_shipments_data_chain' compiled successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<a href=\"/pipeline/#/pipelines/details/d8f13d9c-753f-420b-ab27-c39a0be1dbe4\" target=\"_blank\" >Pipeline details</a>."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline chain 'chain_shipments_data_chain' uploaded successfully.\n",
      "All pipeline chains compiled successfully!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:16:05.156555Z",
     "start_time": "2024-12-13T13:16:05.154498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # TEST\n",
    "# @dsl.pipeline(\n",
    "#     name=\"test_data_processing_pipeline\",\n",
    "#     description=\"A pipeline for processing data\"\n",
    "# )\n",
    "# def data_processing_pipeline():\n",
    "#     # Define the pipeline flow using the >> operator\n",
    "#     filtered_data = filtering(data_path='http://csv-reader-service.kubeflow.svc.cluster.local:5000/read_csv',operation='greater_than',column_name='age',threshold=30)\n",
    "#     json_compressed = compress_json_to_gzip(input_json=filtered_data.outputs['output_json'])\n",
    "#     #csv_compressed = compress_csv_to_gzip(...)\n",
    "# \n",
    "#     # Chaining components using the >> operator\n",
    "#     #filtered_data >> json_compressed\n",
    "# \n",
    "# # Compile the pipeline\n",
    "# if __name__ == '__main__':\n",
    "#     from kfp.compiler import Compiler\n",
    "#     client = kfp.Client() \n",
    "#     Compiler().compile(data_processing_pipeline, 'test_data_processing_pipeline.yaml')\n",
    "#     client.upload_pipeline(pipeline_package_path=\"test_data_processing_pipeline.yaml\", pipeline_name=\"test_data_processing_pipeline\")\n",
    "#     print(f\"Pipeline test is uploaded successfully.\")    "
   ],
   "id": "540626630940c2b8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#TEST THE URL :\n",
    "from kfp import dsl\n",
    "\n",
    "@dsl.pipeline(name=\"Long-running Background Python Example\")\n",
    "def pipeline_with_background_service():\n",
    "    keep_alive_step = dsl.ContainerOp(\n",
    "        name=\"background-python-service\",\n",
    "        image=\"myrepo/my-python-service:latest\",\n",
    "        command=[\"sh\", \"-c\"],\n",
    "        arguments=[\n",
    "            \"python my_program.py & echo 'Background process started' && sleep 5\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    next_step = dsl.ContainerOp(\n",
    "        name=\"next-task\",\n",
    "        image=\"python:3.8\",\n",
    "        command=[\"python\", \"-c\"],\n",
    "        arguments=[\"print('This will run after starting the background task.')\"]\n",
    "    )\n",
    "    next_step.after(keep_alive_step)\n"
   ],
   "id": "3095cec76a38950f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
