{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:50:30.087608Z",
     "start_time": "2024-11-01T12:50:30.082658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TODO s : \n",
    "#There must be no chain pipeline in the input original file.\n",
    "# common stages are choose only by bing the exact same and not the one with higher data access\n",
    "# no policy enforcement applied\n"
   ],
   "id": "49ad06990ff22312",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:50:31.789173Z",
     "start_time": "2024-11-01T12:50:30.096539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install ipynb\n",
    "!pip install ruamel.yaml\n",
    "#!pip install import-ipynb"
   ],
   "id": "9b50c1f8ccfb3dce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipynb in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (0.5.1)\r\n",
      "Requirement already satisfied: ruamel.yaml in /Users/sepideh.masoudi/miniconda3/lib/python3.12/site-packages (0.17.21)\r\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:50:31.796892Z",
     "start_time": "2024-11-01T12:50:31.794779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Tuple\n",
    "from ipynb.fs.full.kubeflowPipelineGenerator import * \n",
    "from difflib import SequenceMatcher\n",
    "import uuid"
   ],
   "id": "10b8117a1027d5c9",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:50:31.806958Z",
     "start_time": "2024-11-01T12:50:31.803866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_match_stages(pipeline_config: PipelineConfig) -> List[Tuple[List[List[str]], Pipeline, Pipeline]]:\n",
    "    common_stages_list = []  # To store the result\n",
    "\n",
    "    # Get all pipelines from the config\n",
    "    pipelines = pipeline_config.pipelines\n",
    "\n",
    "    # Iterate over every pair of pipelines\n",
    "    for i in range(len(pipelines)):\n",
    "        for j in range(i + 1, len(pipelines)):\n",
    "            pipeline1 = pipelines[i]\n",
    "            pipeline2 = pipelines[j]\n",
    "\n",
    "            # Find the common stages in the same sequence using SequenceMatcher\n",
    "            common_stages = find_contiguous_matches(pipeline1.flow, pipeline2.flow)\n",
    "\n",
    "            # Only add non-empty common stages\n",
    "            if common_stages:\n",
    "                common_stages_list.append((common_stages, pipeline1, pipeline2))\n",
    "\n",
    "    return common_stages_list\n",
    "\n",
    "def find_contiguous_matches(flow1: List[str], flow2: List[str]) -> List[List[str]]:\n",
    "    matcher = SequenceMatcher(None, flow1, flow2)\n",
    "    common_stages = []\n",
    "\n",
    "    for block in matcher.get_matching_blocks():\n",
    "        if block.size > 0:  # Only consider blocks with matches\n",
    "            start1 = block.a\n",
    "            length = block.size\n",
    "\n",
    "            # Collect the matched stages from flow1\n",
    "            matched_stages = flow1[start1:start1 + length]\n",
    "\n",
    "            # Only add matched stages as a separate list\n",
    "            common_stages.append(matched_stages)\n",
    "\n",
    "    return common_stages\n",
    "\n"
   ],
   "id": "b389ec62224d070",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:50:31.813890Z",
     "start_time": "2024-11-01T12:50:31.811205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# def filter_common_stages_by_length(threshold: int, common_stages: List[Tuple[List[List[str]], Pipeline, Pipeline]]) -> List[Tuple[List[List[str]], Pipeline, Pipeline]]:\n",
    "#     \"\"\"\n",
    "#     Filters the common stages to only include those with sublist lengths greater than the threshold.\n",
    "#     \"\"\"\n",
    "#     return [\n",
    "#         (stages, pipeline1, pipeline2)\n",
    "#         for stages, pipeline1, pipeline2 in common_stages\n",
    "#         if any(len(sublist) > threshold for sublist in stages)  # Check for any sublist longer than the threshold\n",
    "#     ]\n",
    "def filter_common_stages_by_length(threshold: int, common_stages: List[Tuple[List[List[str]], Pipeline, Pipeline]]) -> List[Tuple[List[List[str]], Pipeline, Pipeline]]:\n",
    "    \"\"\"\n",
    "    Filters the common stages to only include those sublists where their length is greater than the threshold.\n",
    "    \"\"\"\n",
    "    filtered_stages = []\n",
    "    \n",
    "    for stages, pipeline1, pipeline2 in common_stages:\n",
    "        # Keep only sublists with length greater than the threshold\n",
    "        valid_stages = [sublist for sublist in stages if len(sublist) > threshold]\n",
    "        \n",
    "        # If there are any valid stages, add them to the filtered list\n",
    "        if valid_stages:\n",
    "            filtered_stages.append((valid_stages, pipeline1, pipeline2))\n",
    "\n",
    "    return filtered_stages\n"
   ],
   "id": "a33efffde029bb1",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:50:31.820163Z",
     "start_time": "2024-11-01T12:50:31.818521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#create new pipelines from common stages\n",
    "\n",
    "# def create_new_pipeline(filtered_common_stages:List[Tuple[List[List[str]], Pipeline, Pipeline]]) -> Pipeline :\n",
    "#     \n",
    "#     new_pipeline_list =[] \n",
    "#     \n",
    "#     for common_stage_names, pipeline1_obj, pipeline2_obj in filtered_common_stages:\n",
    "#         for sublist in common_stage_names:\n",
    "#             flow = [sublist]\n",
    "#             consumers= [pipeline1_obj.consumers, pipeline2_obj.consumers]\n",
    "#             datasource = Datasource(type=\"pipeline\", metadata=pipeline1_obj.name+','+pipeline2_obj.name)\n",
    "#             name='auto_generated_'+str(uuid.uuid4())\n",
    "#             new_pipeline = Pipeline(name=name, flow=flow, datasource=datasource, consumers=consumers)\n",
    "#             new_pipeline_list.append(new_pipeline)\n",
    "#     return new_pipeline_list"
   ],
   "id": "dbe32fbfa5b8d316",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:50:31.827123Z",
     "start_time": "2024-11-01T12:50:31.824848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_list_by_common_parts(pipeline_flow_list: List[str], common_sub_flow_list:List[List[str]])-> List[List[str]]:\n",
    "    result = []\n",
    "    i = 0\n",
    "\n",
    "    for common_sublist in common_sub_flow_list:\n",
    "        # Find the index of the common sublist in the main list\n",
    "        start_idx = pipeline_flow_list.index(common_sublist[0], i)\n",
    "        end_idx = start_idx + len(common_sublist)\n",
    "\n",
    "        # Add the part before the common sublist, if it exists\n",
    "        if start_idx > i:\n",
    "            result.append(pipeline_flow_list[i:start_idx])\n",
    "\n",
    "        # Add the common sublist itself\n",
    "        result.append(pipeline_flow_list[start_idx:end_idx])\n",
    "\n",
    "        # Update the index to continue after the common sublist\n",
    "        i = end_idx\n",
    "\n",
    "    # Add the remaining part of the list, if it exists\n",
    "    if i < len(pipeline_flow_list):\n",
    "        result.append(pipeline_flow_list[i:])\n",
    "\n",
    "    return result\n",
    "    \n",
    "        "
   ],
   "id": "fa94b22c4abed639",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:50:31.840888Z",
     "start_time": "2024-11-01T12:50:31.836025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_new_pipelines(filtered_common_stages: List[Tuple[List[List[str]], Pipeline, Pipeline]]) -> Tuple[List[Pipeline], List[PipelineChain]]:\n",
    "    \n",
    "    pipeline_list = []\n",
    "    pipeline_chain_list = []\n",
    "\n",
    "    \n",
    "    for common_stage_lists, pipeline1_obj, pipeline2_obj in filtered_common_stages:\n",
    "        pipeline_1_chain= PipelineChain(name=pipeline1_obj.name+'_'+ str(uuid.uuid4()),flow=[])\n",
    "        pipeline_2_chain= PipelineChain(name=pipeline2_obj.name+'_'+ str(uuid.uuid4()),flow=[])\n",
    "    \n",
    "        pipeline1_sub_flows=split_list_by_common_parts(pipeline1_obj.flow,common_stage_lists)\n",
    "        pipeline2_sub_flows=split_list_by_common_parts(pipeline2_obj.flow,common_stage_lists)\n",
    "        \n",
    "################################################################## pipeline_1_flow\n",
    "        for flow in pipeline1_sub_flows:\n",
    "            if flow not in common_stage_lists:\n",
    "                 # Check if the flow exists in the pipeline_list\n",
    "                pipeline_found = False\n",
    "                for pipeline in pipeline_list:\n",
    "                    if pipeline.flow == flow:\n",
    "                        pipeline_1_chain.flow.append(pipeline.name)\n",
    "                        pipeline_found = True\n",
    "                        break\n",
    "                \n",
    "                # If pipeline with the flow wasn't found, create a new one and add it to the chain\n",
    "                if not pipeline_found:\n",
    "                    new_pipeline=Pipeline(\n",
    "                        name=f\"auto_gen_pipeline_{uuid.uuid4()}\",\n",
    "                        flow=flow,\n",
    "                        datasource=Datasource(type=\"pipeline\", metadata=pipeline1_obj.name),\n",
    "                        consumers=pipeline1_obj.consumers\n",
    "                    )\n",
    "                    pipeline_list.append(new_pipeline)\n",
    "                    \n",
    "                    pipeline_1_chain.flow.append(new_pipeline.name)\n",
    "            else:\n",
    "                # Check if the flow exists in the pipeline_list\n",
    "                pipeline_found = False\n",
    "                for pipeline in pipeline_list:\n",
    "                    if pipeline.flow == flow:\n",
    "                        pipeline_1_chain.flow.append(pipeline.name)\n",
    "                        pipeline_found = True\n",
    "                        break\n",
    "                \n",
    "                # If pipeline with the flow wasn't found, create a new one and add it to the chain\n",
    "                if not pipeline_found:\n",
    "                    if isinstance(pipeline1_obj.consumers, str):\n",
    "                        pipeline1_obj.consumers = [pipeline1_obj.consumers]\n",
    "                    if isinstance(pipeline2_obj.consumers, str):\n",
    "                        pipeline2_obj.consumers = [pipeline2_obj.consumers]   \n",
    "                    new_pipeline = Pipeline(\n",
    "                    name=f\"auto_gen_pipeline_{uuid.uuid4()}\",\n",
    "                    flow=flow,\n",
    "                    datasource=Datasource(type=\"pipeline\", metadata=pipeline1_obj.name+','+pipeline2_obj.name),\n",
    "                    consumers=pipeline1_obj.consumers.extend(pipeline2_obj.consumers)\n",
    "                     )\n",
    "                    pipeline_list.append(new_pipeline)\n",
    "                    pipeline_1_chain.flow.append(new_pipeline.name)\n",
    "\n",
    "        pipeline_chain_list.append(pipeline_1_chain)\n",
    "        \n",
    "########################################################### pipeline_2_flow\n",
    "        for flow in pipeline2_sub_flows:\n",
    "            if flow not in common_stage_lists:\n",
    "                # Check if the flow exists in the pipeline_list\n",
    "                pipeline_found = False\n",
    "                for pipeline in pipeline_list:\n",
    "                    if pipeline.flow == flow:\n",
    "                        pipeline_2_chain.flow.append(pipeline.name)\n",
    "                        pipeline_found = True\n",
    "                        break\n",
    "                \n",
    "                # If pipeline with the flow wasn't found, create a new one and add it to the chain\n",
    "                if not pipeline_found:\n",
    "                    new_pipeline=Pipeline(\n",
    "                        name=f\"auto_gen_pipeline_{uuid.uuid4()}\",\n",
    "                        flow=flow,\n",
    "                        datasource=Datasource(type=\"pipeline\", metadata=pipeline2_obj.name),\n",
    "                        consumers=pipeline2_obj.consumers\n",
    "                    )\n",
    "                    pipeline_list.append(new_pipeline)\n",
    "                    pipeline_2_chain.flow.append(new_pipeline.name)\n",
    "            else:\n",
    "                # Check if the flow exists in the pipeline_list\n",
    "                pipeline_found = False\n",
    "                for pipeline in pipeline_list:\n",
    "                    if pipeline.flow == flow:\n",
    "                        pipeline_2_chain.flow.append(pipeline.name)\n",
    "                        pipeline_found = True\n",
    "                        break\n",
    "                \n",
    "                # If pipeline with the flow wasn't found, create a new one and add it to the chain\n",
    "                if not pipeline_found:\n",
    "                    if isinstance(pipeline1_obj.consumers, str):\n",
    "                        pipeline1_obj.consumers = [pipeline1_obj.consumers]\n",
    "                    if isinstance(pipeline2_obj.consumers, str):\n",
    "                        pipeline2_obj.consumers = [pipeline2_obj.consumers]  \n",
    "                        \n",
    "                    new_pipeline = Pipeline(\n",
    "                    name=f\"auto_gen_pipeline_{uuid.uuid4()}\",\n",
    "                    flow=flow,\n",
    "                    datasource=Datasource(type=\"pipeline\", metadata=pipeline1_obj.name+','+pipeline2_obj.name),\n",
    "                    consumers=pipeline1_obj.consumers.extend(pipeline2_obj.consumers)\n",
    "                     )\n",
    "                    pipeline_list.append(new_pipeline)\n",
    "                    pipeline_2_chain.flow.append(new_pipeline.name)\n",
    "                    \n",
    "        pipeline_chain_list.append(pipeline_2_chain)   \n",
    "        \n",
    "    return pipeline_list, pipeline_chain_list                        "
   ],
   "id": "611da8ffc98006ae",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:50:31.854897Z",
     "start_time": "2024-11-01T12:50:31.852483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "class NoAliasDumper(yaml.Dumper):\n",
    "    def ignore_aliases(self, data):\n",
    "        return True\n",
    "    \n",
    "def write_pipeline_config_to_yaml(config: PipelineConfig, path: str, filename: str):\n",
    "    BASE_DATA_DIR = 'data/'\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    full_dir_path = os.path.join(BASE_DATA_DIR, path)  # Path where the YAML will be saved\n",
    "    os.makedirs(full_dir_path, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    \n",
    "    # Construct the full file path (ensure filename is valid)\n",
    "    full_file_path = os.path.join(full_dir_path, filename)  # Complete file path\n",
    "    \n",
    "    # Write the YAML file\n",
    "    with open(full_file_path, 'w') as file:\n",
    "        yaml.dump(config.to_dict(), file, default_flow_style=False, allow_unicode=True,Dumper=NoAliasDumper,sort_keys=False)"
   ],
   "id": "146a5463c58fe4f0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:50:31.864706Z",
     "start_time": "2024-11-01T12:50:31.862301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# keep pipeline that has nothing in common\n",
    "# generate a chain fo them to\n",
    "# create the new pipeline config obj\n",
    "def regenerate_pipeline_config_yml(pipeline_config:PipelineConfig,filtered_common_stages: List[Tuple[List[List[str]], Pipeline, Pipeline]],path:str, filename: str)-> Tuple[List[Pipeline], List[PipelineChain]]:\n",
    "    \n",
    "    new_pipeline_list, pipeline_chain_list = generate_new_pipelines(filtered_common_stages) \n",
    "    \n",
    "    for pipeline in pipeline_config.pipelines:\n",
    "        pipeline_exist = False\n",
    "        for common_stage,pipeline_1,pipeline_2 in filtered_common_stages:\n",
    "            if pipeline==pipeline_1 or pipeline_2 == pipeline :\n",
    "                pipeline_exist = True\n",
    "                break\n",
    "        if not pipeline_exist:\n",
    "            new_pipeline_list.append(pipeline)\n",
    "            pipeline_chain_list.append(PipelineChain(name=pipeline.name , flow=pipeline.name))\n",
    "    \n",
    "    pipeline_config.pipelines = new_pipeline_list\n",
    "    pipeline_config.pipeline_chains = pipeline_chain_list\n",
    "    write_pipeline_config_to_yaml(pipeline_config,path,filename) \n",
    "    \n",
    "    return new_pipeline_list, pipeline_chain_list"
   ],
   "id": "c0199fa6aa10039b",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T12:50:31.871235Z",
     "start_time": "2024-11-01T12:50:31.868396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Iterate over the config file to find common stages between pipelines\n",
    "def parse_pipeline_config(config_path:str, new_config_path:str, new_config_name:str):\n",
    "    \n",
    "    pipeline_config =read_yaml_to_pipeline_config(config_path)\n",
    "    \n",
    "    common_stages_tuple= find_match_stages(pipeline_config)\n",
    "    \n",
    "    filtered_common_stages=filter_common_stages_by_length(0,common_stages_tuple)\n",
    "    for common_stage_names, pipeline1_obj, pipeline2_obj in filtered_common_stages:\n",
    "        print(f\"Common stages between {pipeline1_obj.name} and {pipeline2_obj.name}: {common_stage_names}\")\n",
    "        \n",
    "########### DEBUG #####################\n",
    "    for common_stage_lists, pipeline1_obj, pipeline2_obj in filtered_common_stages:\n",
    "\n",
    "        print(f\"original_pipeline_1 :  name = {pipeline1_obj.name} , flow: {pipeline1_obj.flow}\") \n",
    "        print(f\"original_pipeline_2 :  name = {pipeline2_obj.name} , flow: {pipeline2_obj.flow}\") \n",
    "        print(f\"common_stages_pipelines : {common_stage_lists}\") \n",
    "\n",
    "        test = split_list_by_common_parts(pipeline1_obj.flow,common_stage_lists)        \n",
    "        test_2 = split_list_by_common_parts(pipeline2_obj.flow,common_stage_lists)  \n",
    "\n",
    "        for list in test:\n",
    "            print(f\"split_list_by_common_parts_pipeline_1 : {list}\")\n",
    "        for list in test_2:\n",
    "            print(f\"split_list_by_common_parts_pipeline_2 : {list}\")\n",
    "        \n",
    "######################################    \n",
    "    \n",
    "    new_pipeline_list, pipeline_chain_list = regenerate_pipeline_config_yml(pipeline_config,filtered_common_stages,new_config_path,new_config_name)\n",
    "    \n",
    "    for new_pipeline in new_pipeline_list:\n",
    "        print(f\"New auto-generated pipeline is {new_pipeline.name}: ,  stages are: {new_pipeline.flow}\")\n",
    "\n",
    "    for chain in pipeline_chain_list:\n",
    "        print(f\"New auto-generated chains are {chain.name}: pipelines are: {chain.flow}\")        \n",
    "\n",
    "    # create new pipeline config file\n",
    "    \n",
    "\n",
    "    return"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:50:31.881179Z",
     "start_time": "2024-11-01T12:50:31.874688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \n",
    "    parse_pipeline_config(config_path='data/shipments_pipeline_config.yaml',new_config_path='new_pipeline', new_config_name='shipments_pipeline_config.yaml')\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "f240066c044e212a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common stages between employee_data_pipeline and sales_data_pipeline: [['filter_data'], ['aggregate_data', 'compress_json_to_output']]\n",
      "original_pipeline_1 :  name = employee_data_pipeline , flow: ['filter_data', 'anonymize_columns', 'aggregate_data', 'compress_json_to_output']\n",
      "original_pipeline_2 :  name = sales_data_pipeline , flow: ['filter_data', 'aggregate_data', 'compress_json_to_output']\n",
      "common_stages_pipelines : [['filter_data'], ['aggregate_data', 'compress_json_to_output']]\n",
      "split_list_by_common_parts_pipeline_1 : ['filter_data']\n",
      "split_list_by_common_parts_pipeline_1 : ['anonymize_columns']\n",
      "split_list_by_common_parts_pipeline_1 : ['aggregate_data', 'compress_json_to_output']\n",
      "split_list_by_common_parts_pipeline_2 : ['filter_data']\n",
      "split_list_by_common_parts_pipeline_2 : ['aggregate_data', 'compress_json_to_output']\n",
      "New auto-generated pipeline is auto_gen_pipeline_be167e83-d098-4252-9933-8f914c998daf: ,  stages are: ['filter_data']\n",
      "New auto-generated pipeline is auto_gen_pipeline_811b7ffc-a666-468e-8d4d-0a07e2a96090: ,  stages are: ['anonymize_columns']\n",
      "New auto-generated pipeline is auto_gen_pipeline_27c0424e-2265-4a77-bbbb-a026ddea1c83: ,  stages are: ['aggregate_data', 'compress_json_to_output']\n",
      "New auto-generated chains are employee_data_pipeline_618e4a19-530b-405b-9ce6-a1f7788b9778: pipelines are: ['auto_gen_pipeline_be167e83-d098-4252-9933-8f914c998daf', 'auto_gen_pipeline_811b7ffc-a666-468e-8d4d-0a07e2a96090', 'auto_gen_pipeline_27c0424e-2265-4a77-bbbb-a026ddea1c83']\n",
      "New auto-generated chains are sales_data_pipeline_764bd130-0123-4a60-9f4e-31ff2b14b47f: pipelines are: ['auto_gen_pipeline_be167e83-d098-4252-9933-8f914c998daf', 'auto_gen_pipeline_27c0424e-2265-4a77-bbbb-a026ddea1c83']\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
